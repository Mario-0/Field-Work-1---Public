{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BootMR\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\BootMR\\Documents\\data_export\n"
     ]
    }
   ],
   "source": [
    "import neurokit2 as nk\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import xgboost as xgb\n",
    "import shap\n",
    "import scipy.stats as stats\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "from scipy.stats import linregress, ttest_rel, wilcoxon, zscore, skew, kurtosis\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import warnings\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Specify the path to the desired directory\n",
    "parent_dir = r'<<< PLACE HERE DIRECTORY WITH DATASET >>>'\n",
    "\n",
    "# Change the current working directory to the specified directory\n",
    "os.chdir(parent_dir)\n",
    "\n",
    "#all_ratingsflirtneurokit = pd.read_csv(\"all_ratingsflirtneurokit.csv\")\n",
    "#all_ratings = pd.read_csv(\"all_merged_cleaned_wspeed.csv\")\n",
    "\n",
    "\n",
    "#responses = pd.read_excel(\"responses-sociodem.xlsx\")\n",
    "mastertimesheet = pd.read_excel(\"mastertimesheet-4.xlsx\")\n",
    "time_cols = ['startt1', 'stopt1', 'startt2', 'stopt2', 'startt3', 'stopt3']\n",
    "mastertimesheet[time_cols] = mastertimesheet[time_cols].apply(pd.to_datetime)\n",
    "\n",
    "sociodem = pd.read_excel(\"responses-full-cleaned.xlsx\")\n",
    "\n",
    "# Verify that the working directory has been changed\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "# Function to load file into a DataFrame\n",
    "def load_file_into_dataframe(folder_path, var, filetype, sep=','):\n",
    "    var_files = [f for f in os.listdir(folder_path) if f.endswith(filetype) and var in f]\n",
    "    \n",
    "    if var_files:\n",
    "        file_path = os.path.join(folder_path, var_files[0])\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, sep=sep)\n",
    "            #print(f\"Loaded file: {file_path}\")\n",
    "\n",
    "            # Check if 'p_id' column exists\n",
    "            if 'p_id' in df.columns:\n",
    "                # Add leading zeros to numbers between 0 and 10 in the 'p_id' column\n",
    "                df['p_id'] = df['p_id'].apply(lambda x: f'{int(x):02d}' if isinstance(x, (int, float)) and 0 <= int(x) < 10 else x)\n",
    "                #print(\"Added leading zeros to 'p_id' column for numbers between 0 and 10.\")\n",
    "\n",
    "            return df\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"The file {file_path} is empty.\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred while reading the file {file_path}: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        #print(f\"No file with '{var}' in its name found in folder {folder_path}.\")\n",
    "        return None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Suppress specific runtime warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, message=\".*mean of empty slice.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, message=\".*invalid value encountered in divide.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, message=\".*Degrees of freedom <= 0 for slice.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, message=\".*divide by zero encountered in divide.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, message=\".*invalid value encountered in multiply.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, message=\".*invalid value encountered in scalar divide.*\")\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, message=\".*Precision loss occurred.*\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clean columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: C:\\Users\\BootMR\\Documents\\data_export\\00-code_export\n",
      "ratings_features missing or empty in folder C:\\Users\\BootMR\\Documents\\data_export\\00-code_export.\n",
      "Processing folder: C:\\Users\\BootMR\\Documents\\data_export\\03\n",
      "Saved merged features to: C:\\Users\\BootMR\\Documents\\data_export\\03\\03_ratingsFeatures_baselcorr_17-3_C1.csv\n",
      "Processing folder: C:\\Users\\BootMR\\Documents\\data_export\\04\n",
      "ratings_features missing or empty in folder C:\\Users\\BootMR\\Documents\\data_export\\04.\n"
     ]
    }
   ],
   "source": [
    "# worked well 17-3\n",
    "\n",
    "skip_p_ids = [f\"{i:02}\" for i in range(1)]\n",
    "\n",
    "\n",
    "# Iterate through each subfolder in the root folder\n",
    "for p_id in os.listdir(parent_dir):\n",
    "    folder_path = os.path.join(parent_dir, p_id)\n",
    "    \n",
    "    if p_id in skip_p_ids:\n",
    "        print(f\"Skipping folder as instructed: {folder_path} (p_id {p_id})\")\n",
    "        continue\n",
    "    \n",
    "    if os.path.isdir(folder_path):\n",
    "        \n",
    "        print(f\"Processing folder: {folder_path}\")\n",
    "        \n",
    "        # Initialize an empty DataFrame to store the results for the current p_id\n",
    "        button_features = pd.DataFrame()\n",
    "        \n",
    "        # Load ECG and buttons data\n",
    "         \n",
    "        ratings_features = load_file_into_dataframe(folder_path, 'ratingsFeatures_baselcorr_17-3', '.csv', ',')\n",
    "\n",
    "        if ratings_features is None or ratings_features.empty:\n",
    "            print(f\"ratings_features missing or empty in folder {folder_path}.\")\n",
    "            continue\n",
    "        \n",
    "        #if ratings_wHRV is None or ratings_wHRV.empty:\n",
    "        #    print(f\"ratings_wHRV missing or empty in folder {folder_path}.\")\n",
    "        #    continue\n",
    "        \n",
    "        if ratings_features is not None:\n",
    "\n",
    "            ####### remove and reorder columns\n",
    "\n",
    "            # 1. Delete specified columns\n",
    "            columns_to_delete = ['hr_mean.1', 'hr_stdev.1', 'hrv_mean.1', 'hrv_stdev.1', 'time_diff']\n",
    "            ratings_features.drop(columns=columns_to_delete, inplace=True, errors='ignore')\n",
    "\n",
    "            # 2. Add prefix \"H10_\" to specific columns\n",
    "            columns_to_prefix = [\"hr_stdev\", \"hr_mean\", \"hrv_mean\", \"hrv_stdev\"]\n",
    "            ratings_features.rename(columns={col: f\"H10_{col}\" for col in columns_to_prefix if col in ratings_features.columns}, inplace=True)\n",
    "\n",
    "            # 3. Add suffix \"_H10\" to all columns that start with \"HRV_\"\n",
    "            ratings_features.rename(columns={col: f\"H10_{col}\" for col in ratings_features.columns if col.startswith(\"HRV_\")}, inplace=True)\n",
    "\n",
    "            # 4. Add suffix \"_E4\" to all columns that start with \"hrv_\", \"eda_\", or \"acc_\"\n",
    "            ratings_features.rename(columns={col: f\"E4_{col}\" for col in ratings_features.columns if col.startswith((\"num_\", \"hrv_\", \"eda_\", \"acc_\"))}, inplace=True)\n",
    "\n",
    "            # 5. Rename \"time_difference\" to \"timediff_buttonloc\"\n",
    "            ratings_features.rename(columns={\"time_difference\": \"timediff_buttonloc\"}, inplace=True)\n",
    "\n",
    "            # 6. Move \"timediff_flirtnk\" to be between \"timediff_buttonloc\" and \"window_start_time\"\n",
    "            columns = list(ratings_features.columns)\n",
    "            if \"timediff_flirtnk\" in columns and \"timediff_buttonloc\" in columns and \"window_start_time\" in columns:\n",
    "                columns.remove(\"timediff_flirtnk\")\n",
    "                timediff_buttonloc_index = columns.index(\"timediff_buttonloc\")\n",
    "                columns.insert(timediff_buttonloc_index + 1, \"timediff_flirtnk\")\n",
    "                ratings_features = ratings_features[columns]\n",
    "\n",
    "            # 7. Move column timestamp_flirt\n",
    "            columns = list(ratings_features.columns)\n",
    "            if \"timestamp_location\" in columns and \"latitude\" in columns and \"timestamp_flirt\" in columns:\n",
    "                columns.remove(\"timestamp_flirt\")\n",
    "                timediff_buttonloc_index = columns.index(\"timestamp_location\")\n",
    "                columns.insert(timediff_buttonloc_index + 1, \"timestamp_flirt\")\n",
    "                ratings_features = ratings_features[columns]\n",
    "\n",
    "            # 8. Rename and reorder first set of descriptive columns\n",
    "\n",
    "            # Rename columns first\n",
    "            rename_mapping = {\n",
    "                \"timediff_buttonloc\": \"timestamp_timediff_buttonloc\",\n",
    "                \"timediff_flirtnk\": \"timestamp_timediff_flirtnk\",\n",
    "                \"window_start_time\": \"timestamp_window_start_time\",\n",
    "                \"window_end_time\": \"timestamp_window_end_time\"\n",
    "            }\n",
    "            ratings_features = ratings_features.rename(columns=rename_mapping)\n",
    "\n",
    "            # Define the desired order (after renaming)\n",
    "            desired_order = [\n",
    "                \"rating\", \n",
    "                \"latitude\", \"longitude\", \n",
    "                \"timestamp_button\", \"timestamp_location\", \"timestamp_flirt\", \n",
    "                \"timestamp_timediff_buttonloc\", \"timediff_flirtnk\", \n",
    "                \"timestamp_window_start_time\", \"timestamp_window_end_time\"\n",
    "            ]\n",
    "\n",
    "            # Ensure all desired columns exist before reordering\n",
    "            existing_columns = list(ratings_features.columns)\n",
    "            ordered_columns = [col for col in desired_order if col in existing_columns] + [\n",
    "                col for col in existing_columns if col not in desired_order\n",
    "            ]\n",
    "\n",
    "            # Apply reordering\n",
    "            ratings_features = ratings_features[ordered_columns]\n",
    "\n",
    "            \n",
    "            # Save the corrected DataFrame to a new CSV file in the same folder\n",
    "            output_file_path = os.path.join(folder_path, f\"{p_id}_ratingsFeatures_baselcorr_17-3_C1.csv\")\n",
    "            ratings_features.to_csv(output_file_path, index=None) \n",
    "            print(f\"Saved merged features to: {output_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add warning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: C:\\Users\\BootMR\\Documents\\data_export\\00-code_export\n",
      "ratings_features missing or empty in folder C:\\Users\\BootMR\\Documents\\data_export\\00-code_export.\n",
      "Processing folder: C:\\Users\\BootMR\\Documents\\data_export\\03\n",
      "Saved merged features to: C:\\Users\\BootMR\\Documents\\data_export\\03\\03_ratingsFeatures_baselcorr_17-3_C1W.csv\n",
      "Processing folder: C:\\Users\\BootMR\\Documents\\data_export\\04\n",
      "ratings_features missing or empty in folder C:\\Users\\BootMR\\Documents\\data_export\\04.\n"
     ]
    }
   ],
   "source": [
    "### worked well 5th Feb. script to add warning data to baseline corrected features\n",
    "######## executed well already at 5th feb; be careful about executing it again as it will give errors\n",
    "# still worked well at 17-3\n",
    "\n",
    "# List of p_ids to skip\n",
    "skip_p_ids = [f\"{i:02}\" for i in range(1)]\n",
    "\n",
    "for p_id in os.listdir(parent_dir):\n",
    "    folder_path = os.path.join(parent_dir, p_id)\n",
    "    \n",
    "    if p_id in skip_p_ids:\n",
    "        print(f\"Skipping folder as instructed: {folder_path} (p_id {p_id})\")\n",
    "        continue\n",
    "    \n",
    "    if os.path.isdir(folder_path):\n",
    "        print(f\"Processing folder: {folder_path}\")\n",
    "        \n",
    "        # Load data\n",
    "        ratings_features = load_file_into_dataframe(folder_path, 'ratingsFeatures_baselcorr_17-3_C1', '.csv', ',')\n",
    "        warning_data = load_file_into_dataframe(folder_path, 'warning_data_merged.csv', '.csv', ',')\n",
    "\n",
    "        if ratings_features is None or ratings_features.empty:\n",
    "            print(f\"ratings_features missing or empty in folder {folder_path}.\")\n",
    "            continue\n",
    "        \n",
    "        if warning_data is None or warning_data.empty:\n",
    "            print(f\"warning_data missing or empty in folder {folder_path}.\")\n",
    "            continue\n",
    "        \n",
    "        if ratings_features is not None and warning_data is not None:\n",
    "            ratings_features['timestamp_button'] = pd.to_datetime(ratings_features['timestamp_button'], utc=True).dt.tz_localize(None)\n",
    "            warning_data['timestamp'] = pd.to_datetime(warning_data['timestamp'], utc=True).dt.tz_localize(None)\n",
    "\n",
    "            for index, row in ratings_features.iterrows():\n",
    "                window_start_time = row['timestamp_window_start_time']\n",
    "                window_end_time = row['timestamp_window_end_time']\n",
    "\n",
    "                # Filter for timestamps within the time window\n",
    "                warnings_in_window = warning_data[\n",
    "                    (warning_data['timestamp'] >= window_start_time) &\n",
    "                    (warning_data['timestamp'] <= window_end_time) &\n",
    "                    (warning_data['warning_type'] == \"SlowDown\")  # Only select \"SlowDown\" warnings\n",
    "                ]\n",
    "\n",
    "                slowdown_count = len(warnings_in_window)\n",
    "                audio_warning = int(warnings_in_window['audio_warning'].any())  \n",
    "                tactile_warning = int(warnings_in_window['tactile_warning'].any())  \n",
    "\n",
    "                # Compute \"warningsonoff\" column\n",
    "                warningsonoff = 1 if (audio_warning or tactile_warning) else 0\n",
    "\n",
    "                # Compute \"warning_type_text\" column\n",
    "                if audio_warning and tactile_warning:\n",
    "                    warning_type_text = \"audio, tactile\"\n",
    "                elif audio_warning:\n",
    "                    warning_type_text = \"audio\"\n",
    "                elif tactile_warning:\n",
    "                    warning_type_text = \"tactile\"\n",
    "                else:\n",
    "                    warning_type_text = \"none\"\n",
    "\n",
    "                # Store unique p_id and round_id values\n",
    "                p_id_values = warnings_in_window['p_id'].unique()\n",
    "                round_id_values = warnings_in_window['round_id'].unique()\n",
    "\n",
    "                p_id_str = \",\".join(map(str, p_id_values)) if p_id_values.size > 0 else \"\"\n",
    "                round_id_str = \",\".join(map(str, round_id_values)) if round_id_values.size > 0 else \"\"\n",
    "\n",
    "                # Update ratings_features\n",
    "                ratings_features.at[index, \"warnings_slowdown_count\"] = slowdown_count\n",
    "                ratings_features.at[index, \"warnings_audio_warning\"] = audio_warning\n",
    "                ratings_features.at[index, \"warnings_tactile_warning\"] = tactile_warning\n",
    "                ratings_features.at[index, \"warnings_onoff\"] = warningsonoff\n",
    "                ratings_features.at[index, \"warnings_type_text\"] = warning_type_text\n",
    "\n",
    "            # Ensure correct column order\n",
    "            desired_columns = list(ratings_features.columns)  \n",
    "\n",
    "            # Find indices of key positions\n",
    "            idx_start = desired_columns.index(\"timestamp_timediff_flirtnk\") + 1\n",
    "            idx_end = desired_columns.index(\"H10_hr_mean\")\n",
    "\n",
    "            # List of new columns to insert\n",
    "            new_columns = [\"warnings_slowdown_count\", \"warnings_audio_warning\", \"warnings_tactile_warning\", \"warnings_onoff\", \"warnings_type_text\"]\n",
    "\n",
    "            # Remove duplicates if they already exist\n",
    "            desired_columns = [col for col in desired_columns if col not in new_columns]\n",
    "\n",
    "            # Reinsert the warning columns at the correct position\n",
    "            reordered_columns = (\n",
    "                desired_columns[:idx_start] + new_columns + desired_columns[idx_start:idx_end] + desired_columns[idx_end:]\n",
    "            )\n",
    "\n",
    "            # Apply the new column order\n",
    "            ratings_features = ratings_features[reordered_columns]\n",
    "\n",
    "            # Save the corrected DataFrame to a new CSV file\n",
    "            output_file_path = os.path.join(folder_path, f\"{p_id}_ratingsFeatures_baselcorr_17-3_C1W.csv\")\n",
    "            ratings_features.to_csv(output_file_path, index=None) \n",
    "            print(f\"Saved merged features to: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add p_id and interval_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: C:\\Users\\BootMR\\Documents\\data_export\\00-code_export\n",
      "⚠️ ratings_features missing or empty in folder C:\\Users\\BootMR\\Documents\\data_export\\00-code_export. Skipping...\n",
      "Processing folder: C:\\Users\\BootMR\\Documents\\data_export\\03\n",
      "Saved merged features to: C:\\Users\\BootMR\\Documents\\data_export\\03\\03_ratingsFeatures_baselcorr_17-3_C1WP.csv\n",
      "Processing folder: C:\\Users\\BootMR\\Documents\\data_export\\04\n",
      "⚠️ ratings_features missing or empty in folder C:\\Users\\BootMR\\Documents\\data_export\\04. Skipping...\n"
     ]
    }
   ],
   "source": [
    "######## executed well already at 5th feb; be careful about executing it again as it will give errors\n",
    "\n",
    "# List of p_ids to skip\n",
    "#skip_p_ids = []  # Add any p_ids you want to skip\n",
    "skip_p_ids = [f\"{i:02}\" for i in range(1)]\n",
    "\n",
    "# Define a function to determine the interval_id for each timestamp\n",
    "def get_interval_id(timestamp, mastertimesheet_row):\n",
    "    if mastertimesheet_row['startt1'] <= timestamp <= mastertimesheet_row['stopt1']:\n",
    "        return 1\n",
    "    elif mastertimesheet_row['startt2'] <= timestamp <= mastertimesheet_row['stopt2']:\n",
    "        return 2\n",
    "    elif mastertimesheet_row['startt3'] <= timestamp <= mastertimesheet_row['stopt3']:\n",
    "        return 3\n",
    "    else:\n",
    "        return np.nan  # If the timestamp doesn't fall in any interval, return NaN\n",
    "\n",
    "# Iterate through each subfolder in the root folder\n",
    "for p_id in os.listdir(parent_dir):\n",
    "    folder_path = os.path.join(parent_dir, p_id)\n",
    "    \n",
    "    if p_id in skip_p_ids:\n",
    "        print(f\"Skipping folder as instructed: {folder_path} (p_id {p_id})\")\n",
    "        continue\n",
    "    \n",
    "    if os.path.isdir(folder_path):\n",
    "        print(f\"Processing folder: {folder_path}\")\n",
    "        \n",
    "        # Load data\n",
    "        ratings_features = load_file_into_dataframe(folder_path, 'ratingsFeatures_baselcorr_17-3_C1W', '.csv', ',')\n",
    "        \n",
    "        # Ensure ratings_features is valid before proceeding\n",
    "        if ratings_features is None or ratings_features.empty:\n",
    "            print(f\"⚠️ ratings_features missing or empty in folder {folder_path}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Convert timestamp column\n",
    "        ratings_features['timestamp_button'] = pd.to_datetime(ratings_features['timestamp_button'], utc=True).dt.tz_localize(None)\n",
    "\n",
    "        # Initialize new columns\n",
    "        ratings_features['p_id'] = p_id  # Assign current p_id\n",
    "        ratings_features['interval_id'] = np.nan  # Initialize interval_id column\n",
    "        \n",
    "        # Compute rating_totalperpid (total ratings per p_id)\n",
    "        total_ratings_per_p_id = len(ratings_features)\n",
    "        ratings_features['rating_totalperpid'] = total_ratings_per_p_id  # Added only once here\n",
    "\n",
    "        # Ensure p_id, interval_id, and rating columns are placed correctly\n",
    "        column_order = ['p_id', 'interval_id', 'rating'] + [col for col in ratings_features.columns if col not in ['p_id', 'interval_id', 'rating']] \n",
    "        column_order.insert(column_order.index('rating') + 1, 'rating_totalperpid')  # Insert 'rating_totalperpid' after 'rating'\n",
    "        \n",
    "        \n",
    "        # Iterate through each row of ratings_features to determine interval_id\n",
    "        for i, rating_row in ratings_features.iterrows():\n",
    "            timestamp = rating_row['timestamp_button']\n",
    "            \n",
    "            # Iterate through mastertimesheet to find the corresponding interval_id\n",
    "            for j, master_row in mastertimesheet.iterrows():\n",
    "                interval_id = get_interval_id(timestamp, master_row)\n",
    "                if not pd.isna(interval_id):  # If an interval was found, assign it and break the loop\n",
    "                    ratings_features.at[i, 'interval_id'] = interval_id\n",
    "                    break\n",
    "\n",
    "        # Apply the new column order\n",
    "        ratings_features = ratings_features[column_order]\n",
    "\n",
    "        # Save the corrected DataFrame to a new CSV file in the same folder\n",
    "        output_file_path = os.path.join(folder_path, f\"{p_id}_ratingsFeatures_baselcorr_17-3_C1WP.csv\")\n",
    "        ratings_features.to_csv(output_file_path, index=None) \n",
    "        print(f\"Saved merged features to: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add cadence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: C:\\Users\\BootMR\\Documents\\data_export\\00-code_export\n",
      "ratings_features missing or empty in folder C:\\Users\\BootMR\\Documents\\data_export\\00-code_export.\n",
      "Processing folder: C:\\Users\\BootMR\\Documents\\data_export\\03\n",
      "cadence missing or empty in folder C:\\Users\\BootMR\\Documents\\data_export\\03. Leaving cadence columns empty.\n",
      "Saved merged features to: C:\\Users\\BootMR\\Documents\\data_export\\03\\03_ratingsFeatures_baselcorr_17-3_C1WPC.csv\n",
      "Processing folder: C:\\Users\\BootMR\\Documents\\data_export\\04\n",
      "ratings_features missing or empty in folder C:\\Users\\BootMR\\Documents\\data_export\\04.\n"
     ]
    }
   ],
   "source": [
    "# List of p_ids to skip\n",
    "skip_p_ids = [f\"{i:02}\" for i in range(1)]\n",
    "\n",
    "# Iterate through each subfolder in the root folder\n",
    "for p_id in os.listdir(parent_dir):\n",
    "    folder_path = os.path.join(parent_dir, p_id)\n",
    "    \n",
    "    if p_id in skip_p_ids:\n",
    "        print(f\"Skipping folder as instructed: {folder_path} (p_id {p_id})\")\n",
    "        continue\n",
    "    \n",
    "    if os.path.isdir(folder_path):\n",
    "        \n",
    "        print(f\"Processing folder: {folder_path}\")\n",
    "                \n",
    "        ratings_features = load_file_into_dataframe(folder_path, 'ratingsFeatures_baselcorr_17-3_C1WP', 'csv', ',')\n",
    "        cadence = load_file_into_dataframe(folder_path, '_cadence_merged', 'csv', ',')\n",
    "\n",
    "        if ratings_features is None or ratings_features.empty:\n",
    "            print(f\"ratings_features missing or empty in folder {folder_path}.\")\n",
    "            continue\n",
    "\n",
    "        if cadence is None or cadence.empty:\n",
    "            print(f\"cadence missing or empty in folder {folder_path}. Leaving cadence columns empty.\")\n",
    "            # If cadence file is missing, leave cadence columns empty\n",
    "            cadence_columns = [\n",
    "                \"cadence_avg\", \"cadence_min\", \"cadence_max\", \"cadence_std\", \n",
    "                \"cadence_avgacc\", \"cadence_slope\", \"cadence_cv\", \n",
    "                \"cadence_skewness\", \"cadence_kurtosis\", \"cadence_pvr\", \n",
    "                \"cadence_autocorrelation\"\n",
    "            ]\n",
    "            for col in cadence_columns:\n",
    "                ratings_features[col] = np.nan  # Set cadence columns to NaN (empty)\n",
    "        else:\n",
    "            cadence['timestamp'] = pd.to_datetime(cadence['timestamp'])\n",
    "            results = []\n",
    "            \n",
    "            # Loop through each rating in the selected DataFrame\n",
    "            for index, row in ratings_features.iterrows():\n",
    "                window_start_time = row['timestamp_window_start_time']\n",
    "                window_end_time = row['timestamp_window_end_time']\n",
    "\n",
    "                cadence_in_window = cadence[(cadence['timestamp'] >= window_start_time) &\n",
    "                                            (cadence['timestamp'] <= window_end_time)]\n",
    "\n",
    "                avg_cadence = round(cadence_in_window['cadence_tpmn'].mean(), 1)\n",
    "                min_cadence = round(cadence_in_window['cadence_tpmn'].min(), 1)\n",
    "                max_cadence = round(cadence_in_window['cadence_tpmn'].max(), 1)\n",
    "                std_cadence = round(cadence_in_window['cadence_tpmn'].std(), 1)\n",
    "\n",
    "                cadence_diff = cadence_in_window['cadence_tpmn'].diff()\n",
    "                avg_acceleration = round(cadence_diff.mean(), 1)\n",
    "\n",
    "                cadence_in_window_clean = cadence_in_window.dropna(subset=['cadence_tpmn'])\n",
    "                if not cadence_in_window_clean.empty:\n",
    "                    time = np.arange(len(cadence_in_window_clean)).reshape(-1, 1)\n",
    "                    from sklearn.linear_model import LinearRegression\n",
    "                    model = LinearRegression().fit(time, cadence_in_window_clean['cadence_tpmn'])\n",
    "                    slope = round(model.coef_[0], 3)\n",
    "                else:\n",
    "                    slope = np.nan\n",
    "\n",
    "                cv_cadence = round(cadence_in_window['cadence_tpmn'].std() / cadence_in_window['cadence_tpmn'].mean() * 100, 1)\n",
    "\n",
    "                from scipy.stats import skew, kurtosis\n",
    "                cadence_skewness = round(skew(cadence_in_window['cadence_tpmn']), 2)\n",
    "                cadence_kurtosis = round(kurtosis(cadence_in_window['cadence_tpmn']), 2)\n",
    "\n",
    "                peaks = cadence_in_window['cadence_tpmn'][cadence_in_window['cadence_tpmn'] == cadence_in_window['cadence_tpmn'].rolling(3).max()]\n",
    "                valleys = cadence_in_window['cadence_tpmn'][cadence_in_window['cadence_tpmn'] == cadence_in_window['cadence_tpmn'].rolling(3).min()]\n",
    "                peak_valley_ratio = round(len(peaks) / len(valleys) if len(valleys) > 0 else 0, 2)\n",
    "\n",
    "                autocorr = round(np.corrcoef(cadence_in_window['cadence_tpmn'], cadence_in_window['cadence_tpmn'].shift(1))[0, 1], 2)\n",
    "\n",
    "                # Append the results to ratings_features\n",
    "                ratings_features.at[index, \"cadence_avg\"] = avg_cadence\n",
    "                ratings_features.at[index, \"cadence_min\"] = min_cadence\n",
    "                ratings_features.at[index, \"cadence_max\"] = max_cadence\n",
    "                ratings_features.at[index, \"cadence_std\"] = std_cadence\n",
    "                ratings_features.at[index, \"cadence_avgacc\"] = avg_acceleration\n",
    "                ratings_features.at[index, \"cadence_slope\"] = slope\n",
    "                ratings_features.at[index, \"cadence_cv\"] = cv_cadence\n",
    "                ratings_features.at[index, \"cadence_skewness\"] = cadence_skewness\n",
    "                ratings_features.at[index, \"cadence_kurtosis\"] = cadence_kurtosis\n",
    "                ratings_features.at[index, \"cadence_pvr\"] = peak_valley_ratio\n",
    "                ratings_features.at[index, \"cadence_autocorrelation\"] = autocorr\n",
    "\n",
    "        # Save the corrected DataFrame to a new CSV file in the same folder\n",
    "        output_file_path = os.path.join(folder_path, f\"{p_id}_ratingsFeatures_baselcorr_17-3_C1WPC.csv\")\n",
    "        ratings_features.to_csv(output_file_path, index=None)\n",
    "        print(f\"Saved merged features to: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: C:\\Users\\BootMR\\Documents\\data_export\\00-code_export\n",
      "ratings_features missing or empty in folder C:\\Users\\BootMR\\Documents\\data_export\\00-code_export.\n",
      "Processing folder: C:\\Users\\BootMR\\Documents\\data_export\\03\n",
      "Saved merged features to: C:\\Users\\BootMR\\Documents\\data_export\\03\\03_ratingsFeatures_baselcorr_17-3_C1WPCS.csv\n",
      "Processing folder: C:\\Users\\BootMR\\Documents\\data_export\\04\n",
      "ratings_features missing or empty in folder C:\\Users\\BootMR\\Documents\\data_export\\04.\n"
     ]
    }
   ],
   "source": [
    "#### TO BE DONE\n",
    "\n",
    "#took script from add cadence, let's re use it\n",
    "\n",
    "\n",
    "# List of p_ids to skip\n",
    "#skip_p_ids = []  # Add any p_ids you want to skip\n",
    "skip_p_ids = [f\"{i:02}\" for i in range(1)]\n",
    "\n",
    "# Iterate through each subfolder in the root folder\n",
    "for p_id in os.listdir(parent_dir):\n",
    "    folder_path = os.path.join(parent_dir, p_id)\n",
    "    \n",
    "    if p_id in skip_p_ids:\n",
    "        print(f\"Skipping folder as instructed: {folder_path} (p_id {p_id})\")\n",
    "        continue\n",
    "    \n",
    "    if os.path.isdir(folder_path):\n",
    "        \n",
    "        print(f\"Processing folder: {folder_path}\")\n",
    "                \n",
    "        ratings_features = load_file_into_dataframe(folder_path, 'ratingsFeatures_baselcorr_17-3_C1WPC', 'csv', ',')\n",
    "        location_data = load_file_into_dataframe(folder_path, '_location_data_merged', 'csv', ',')\n",
    "\n",
    "        if ratings_features is None or ratings_features.empty:\n",
    "            print(f\"ratings_features missing or empty in folder {folder_path}.\")\n",
    "            continue\n",
    "\n",
    "        if location_data is None or location_data.empty:\n",
    "            print(f\"cadence missing or empty in folder {folder_path}.\")\n",
    "            continue \n",
    "\n",
    "        results = []\n",
    "        \n",
    "        if ratings_features is not None and location_data is not None:\n",
    "\n",
    "            # Loop through each rating in the selected DataFrame\n",
    "            for index, row in ratings_features.iterrows():\n",
    "                # Store window_start_time and window_end_time\n",
    "                window_start_time = row['timestamp_window_start_time']\n",
    "                window_end_time = row['timestamp_window_end_time']\n",
    "\n",
    "                # Filter warnings based on the time window\n",
    "                location_in_window = location_data[(location_data['timestamp'] >= window_start_time) &\n",
    "                                                        (location_data['timestamp'] <= window_end_time)]\n",
    "\n",
    "                if location_in_window.empty:\n",
    "                        continue\n",
    "                \n",
    "                # Velocity Features\n",
    "                avg_velocity = round(location_in_window['velocity'].mean(), 2)\n",
    "                min_velocity = round(location_in_window['velocity'].min(), 2)\n",
    "                max_velocity = round(location_in_window['velocity'].max(), 2)\n",
    "                std_velocity = round(location_in_window['velocity'].std(), 2)\n",
    "                \n",
    "                velocity_diff = location_in_window['velocity'].diff()\n",
    "                avg_velocity_change = round(velocity_diff.mean(), 2)\n",
    "                velocity_cv = round(std_velocity / avg_velocity * 100 if avg_velocity else 0, 2)\n",
    "                \n",
    "                velocity_skewness = round(skew(location_in_window['velocity'].dropna()), 2)\n",
    "                velocity_kurtosis = round(kurtosis(location_in_window['velocity'].dropna()), 2)\n",
    "                \n",
    "                # Linear Regression on Velocity (Trend Analysis)\n",
    "                velocity_clean = location_in_window.dropna(subset=['velocity'])\n",
    "                if not velocity_clean.empty:\n",
    "                    time = np.arange(len(velocity_clean)).reshape(-1, 1)\n",
    "                    model = LinearRegression().fit(time, velocity_clean['velocity'])\n",
    "                    velocity_slope = round(model.coef_[0], 3)\n",
    "                else:\n",
    "                    velocity_slope = np.nan\n",
    "\n",
    "                ratings_features.at[index, \"velocity_avg\"] = avg_velocity\n",
    "                ratings_features.at[index, \"velocity_min\"] = min_velocity\n",
    "                ratings_features.at[index, \"velocity_max\"] = max_velocity\n",
    "                ratings_features.at[index, \"velocity_std\"] = std_velocity\n",
    "                ratings_features.at[index, \"velocity_avg_change\"] = avg_velocity_change\n",
    "                ratings_features.at[index, \"velocity_cv\"] = velocity_cv\n",
    "                ratings_features.at[index, \"velocity_skewness\"] = velocity_skewness\n",
    "                ratings_features.at[index, \"velocity_kurtosis\"] = velocity_kurtosis\n",
    "                ratings_features.at[index, \"velocity_slope\"] = velocity_slope\n",
    "\n",
    "            # Save the corrected DataFrame to a new CSV file in the same folder\n",
    "            output_file_path = os.path.join(folder_path, f\"{p_id}_ratingsFeatures_baselcorr_17-3_C1WPCS.csv\")\n",
    "            ratings_features.to_csv(output_file_path, index=None) \n",
    "            print(f\"Saved merged features to: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add weather? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done and All folders with both files contain the same number of rows.\n"
     ]
    }
   ],
   "source": [
    "### short script to use weather data from initial files, without pulling new requests via API \n",
    "\n",
    "# Variable to track the overall status\n",
    "all_match = True\n",
    "\n",
    "# Iterate through each subfolder\n",
    "for subfolder in os.listdir(parent_dir):\n",
    "    subfolder_path = os.path.join(parent_dir, subfolder)\n",
    "\n",
    "    # Ensure the path is a directory and subfolder is a numeric name\n",
    "    if os.path.isdir(subfolder_path) and subfolder.isdigit():\n",
    "        # Construct file paths\n",
    "        file1 = os.path.join(subfolder_path, f\"{subfolder}_ratingsFeatures_baselcorr_17-3_C1WPCS.csv\")\n",
    "        file2 = os.path.join(subfolder_path, f\"{subfolder}_ratingsFeatures_baselcorr_extended-WEATHER.csv\")\n",
    "\n",
    "        # Check if both files exist in the subfolder\n",
    "        if os.path.exists(file1) and os.path.exists(file2):\n",
    "            # Load the files\n",
    "            try:\n",
    "                df1 = pd.read_csv(file1)\n",
    "                df2 = pd.read_csv(file2)\n",
    "                \n",
    "                # Check row counts\n",
    "                rows_file1 = len(df1)\n",
    "                rows_file2 = len(df2)\n",
    "                \n",
    "                if rows_file1 != rows_file2:\n",
    "                    print(f\"Row count mismatch in subfolder {subfolder}:\")\n",
    "                    print(f\"  {file1} has {rows_file1} rows\")\n",
    "                    print(f\"  {file2} has {rows_file2} rows\")\n",
    "                    all_match = False\n",
    "                else:\n",
    "                    # Merge the two dataframes on index (assuming they correspond row-wise)\n",
    "                    weather_columns = ['temperature', 'wind_speed', 'wind_direction', 'humidity', 'cloudiness', 'rain_3h']\n",
    "                    \n",
    "                    # Add a suffix 'weather_' to the column names\n",
    "                    df2_weather = df2[weather_columns].add_prefix('weather_')\n",
    "                                        \n",
    "                    # Concatenate weather columns to the first dataframe\n",
    "                    df_merged = pd.concat([df1, df2_weather], axis=1)\n",
    "\n",
    "                    # Save the merged dataframe to a new file\n",
    "                    output_file_path = os.path.join(subfolder_path, f\"{subfolder}_ratingsFeatures_baselcorr_17-3_C1WPCSW.csv\")\n",
    "                    df_merged.to_csv(output_file_path, index=False)\n",
    "                    print(f\"Saved merged file with weather data to: {output_file_path}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing files in subfolder {subfolder}: {e}\")\n",
    "        else:\n",
    "            test = 0\n",
    "            #print(f\"Skipping subfolder {subfolder} as one or both files are missing.\")\n",
    "\n",
    "# Final conclusion\n",
    "if all_match:\n",
    "    print(\"Done and All folders with both files contain the same number of rows.\")\n",
    "else:\n",
    "    print(\"Some folders with both files do not contain the same number of rows.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: C:\\Users\\BootMR\\Documents\\data_export\\00-code_export\n",
      "ratings_features missing or empty in folder C:\\Users\\BootMR\\Documents\\data_export\\00-code_export.\n",
      "Processing folder: C:\\Users\\BootMR\\Documents\\data_export\\03\n",
      "52.23802608333334 6.8573067666666665 2024-05-22 11:43:04.127494\n",
      "0 17.75 3.58 273 76 100 0\n",
      "52.231734633333325 6.866081649999999 2024-05-22 11:47:22.196925\n",
      "1 17.75 3.58 273 76 100 0\n",
      "52.23099665 6.859504600000001 2024-05-22 11:50:47.987902\n",
      "2 17.75 3.58 273 76 100 0\n",
      "52.23502611666666 6.858946116666668 2024-05-22 12:06:18.554372\n",
      "3 17.26 4.02 254 77 100 0\n",
      "52.2322988 6.864600983333334 2024-05-22 12:07:35.941486\n",
      "4 17.26 4.02 254 77 100 0\n",
      "52.23219774999999 6.864867783333334 2024-05-22 12:07:38.644613\n",
      "5 17.26 4.02 254 77 100 0\n",
      "52.23620081666666 6.858800366666666 2024-05-22 12:27:01.459779\n",
      "6 17.26 4.02 254 77 100 0\n",
      "52.23226471666667 6.864698216666667 2024-05-22 12:28:42.473445\n",
      "7 17.26 4.02 254 77 100 0\n",
      "Saved merged features to: C:\\Users\\BootMR\\Documents\\data_export\\03\\03_ratingsFeatures_baselcorr_17-3_C1WPCSW.csv\n",
      "Processing folder: C:\\Users\\BootMR\\Documents\\data_export\\04\n",
      "ratings_features missing or empty in folder C:\\Users\\BootMR\\Documents\\data_export\\04.\n"
     ]
    }
   ],
   "source": [
    "# complete script to pull weather data via api for each rating\n",
    "\n",
    "\n",
    "# OpenWeatherMap API key\n",
    "API_KEY = '28269a413589e6c976a00aefd4bdf6b5'\n",
    "BASE_URL = 'https://history.openweathermap.org/data/2.5/history/city'\n",
    "\n",
    "def get_weather_data(lat, lon, timestamp):\n",
    "    # Convert timestamp to Unix time (seconds)\n",
    "    start = int(timestamp.timestamp())\n",
    "    end = start + 3600  # 1 hour later (to ensure data is captured)\n",
    "\n",
    "    url = f\"{BASE_URL}?lat={lat}&lon={lon}&type=hour&start={start}&end={end}&appid={API_KEY}&units=metric\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        if 'list' in data and len(data['list']) > 0:\n",
    "            # Take the closest weather data (assuming sorted by time)\n",
    "            weather = data['list'][0]\n",
    "            temp = weather['main']['temp']\n",
    "            wind_speed = weather['wind']['speed']\n",
    "            wind_direction = weather['wind']['deg']\n",
    "            humidity = weather['main']['humidity']\n",
    "            cloudiness = weather['clouds']['all']\n",
    "            rain = weather.get('rain', {}).get('3h', 0)  # Default to 0 if no rain data\n",
    "\n",
    "            return temp, wind_speed, wind_direction, humidity, cloudiness, rain\n",
    "        else:\n",
    "            return None, None, None, None, None, None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching weather data for {lat}, {lon} at {timestamp}: {e}\")\n",
    "        return None, None, None, None, None, None\n",
    "\n",
    "\n",
    "# List of p_ids to skip\n",
    "#skip_p_ids = []  # Add any p_ids you want to skip\n",
    "skip_p_ids = [f\"{i:02}\" for i in range(1)]\n",
    "\n",
    "# Iterate through each subfolder in the root folder\n",
    "for p_id in os.listdir(parent_dir):\n",
    "    folder_path = os.path.join(parent_dir, p_id)\n",
    "    \n",
    "    if p_id in skip_p_ids:\n",
    "        print(f\"Skipping folder as instructed: {folder_path} (p_id {p_id})\")\n",
    "        continue\n",
    "    \n",
    "    if os.path.isdir(folder_path):\n",
    "        \n",
    "        print(f\"Processing folder: {folder_path}\")\n",
    "                \n",
    "        ratings_features = load_file_into_dataframe(folder_path, 'ratingsFeatures_baselcorr_17-3_C1WPCS', 'csv', ',')\n",
    "\n",
    "        if ratings_features is None or ratings_features.empty:\n",
    "            print(f\"ratings_features missing or empty in folder {folder_path}.\")\n",
    "            continue\n",
    "\n",
    "        ratings_features['timestamp_button'] = pd.to_datetime(ratings_features['timestamp_button'], errors='coerce')\n",
    "\n",
    "        results = []\n",
    "        \n",
    "        if ratings_features is not None:\n",
    "\n",
    "            # Add empty columns for weather data\n",
    "            ratings_features['temperature'] = None\n",
    "            ratings_features['wind_speed'] = None\n",
    "            ratings_features['wind_direction'] = None\n",
    "            \n",
    "            ratings_features['humidity'] = None\n",
    "            ratings_features['cloudiness'] = None\n",
    "            ratings_features['rain_3h'] = None\n",
    "\n",
    "            # Loop through each rating in the selected DataFrame\n",
    "            for index, row in ratings_features.iterrows():\n",
    "\n",
    "                temp, wind_speed, wind_direction, humidity, cloudiness, rain = get_weather_data(row['latitude'], row['longitude'], row['timestamp_button'])\n",
    "\n",
    "                # To avoid exceeding API rate limits\n",
    "                time.sleep(1)\n",
    "                print(row['latitude'], row['longitude'], row['timestamp_button'])\n",
    "                print(index, temp, wind_speed, wind_direction, humidity, cloudiness, rain)\n",
    "\n",
    "                ratings_features.at[index, 'weather_temperature'] = temp\n",
    "                ratings_features.at[index, 'weather_wind_speed'] = wind_speed\n",
    "                ratings_features.at[index, 'weather_wind_direction'] = wind_direction\n",
    "                ratings_features.at[index, 'weather_humidity'] = humidity\n",
    "                ratings_features.at[index, 'weather_cloudiness'] = cloudiness\n",
    "                ratings_features.at[index, 'weather_rain_3h'] = rain\n",
    "\n",
    "\n",
    "            # Save the corrected DataFrame to a new CSV file in the same folder\n",
    "            output_file_path = os.path.join(folder_path, f\"{p_id}_ratingsFeatures_baselcorr_17-3_C1WPCSW.csv\")\n",
    "            ratings_features.to_csv(output_file_path, index=None)\n",
    "            print(f\"Saved merged features to: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add sociodemographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: C:\\Users\\BootMR\\Documents\\data_export\\00-code_export\n",
      "ratings_features missing or empty in folder C:\\Users\\BootMR\\Documents\\data_export\\00-code_export.\n",
      "Processing folder: C:\\Users\\BootMR\\Documents\\data_export\\03\n",
      "Saved merged features to: C:\\Users\\BootMR\\Documents\\data_export\\03\\03_ratingsFeatures_baselcorr_17-3_C1WPCSWS.csv\n",
      "Processing folder: C:\\Users\\BootMR\\Documents\\data_export\\04\n",
      "ratings_features missing or empty in folder C:\\Users\\BootMR\\Documents\\data_export\\04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BootMR\\AppData\\Local\\Temp\\ipykernel_16196\\3679529702.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sociodem_subset.rename(columns=column_name_mapping, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assuming 'P_ID' in sociodem and 'p_id' in ratings_features are the linking columns\n",
    "\n",
    "# Create a dictionary to map the old column names to more meaningful lower case names\n",
    "column_name_mapping = {\n",
    "    'P_ID': 'P_ID',\n",
    "    'Nationality': 'sociodem_nationality',\n",
    "    'gender': 'sociodem_gender',\n",
    "    'age': 'sociodem_age',\n",
    "    'weight': 'sociodem_weight',\n",
    "    'length': 'sociodem_length',\n",
    "    'coffee': 'sociodem_coffee',\n",
    "    'medication': 'sociodem_medication',\n",
    "    'income net': 'sociodem_income',\n",
    "    'education': 'sociodem_education',\n",
    "    'Hoeveel jaar heeft u tenminste eenmaal per jaar in Nederland gefietst?': 'sociodem_cycling_experience',\n",
    "    'Bij hoeveel fietsongevallen met andere voertuigen (bijv. fiets, auto) bent u betrokken geweest in de afgelopen 3 jaar': 'sociodem_bike_accidents',\n",
    "    'E-bike': 'sociodem_ebike',\n",
    "    'safety_predisposition': 'sociodem_safety_predisposition',\n",
    "    'nervousness_predisposition': 'sociodem_nervousness_predisposition',\n",
    "    'digitalDisposition': 'sociodem_digital_disposition',\n",
    "    'arousalDisposition': 'sociodem_arousal_disposition',\n",
    "    'pleasantnessDisposition': 'sociodem_pleasantness_disposition',\n",
    "    'flowDisposition': 'sociodem_flow_disposition', \n",
    "    'context_positivity_1': 'context_positivity_1',\n",
    "    'warning_value_1': 'warning_value_1', \n",
    "    'context_positivity_1': 'context_positivity_1',\n",
    "    'warning_value_2': 'warning_value_2', \n",
    "    'context_positivity_2': 'context_positivity_2',\n",
    "    'warning_value_3': 'warning_value_3', \n",
    "    'context_positivity_3': 'context_positivity_3'\n",
    "}\n",
    "\n",
    "# Define mapping dictionaries\n",
    "column_numerical_mapping = {\n",
    "    \"sociodem_income\": {'0-1000': 0, '1001-1500': 1, '1501-2500': 2, '2501-3000': 3, \n",
    "                '3001-3500': 4, '3501-4000': 5, '4001-4500': 6},\n",
    "\n",
    "    \"sociodem_education\": {'Middle/high school': 0, 'Vocational education': 1, 'Academic education': 2},\n",
    "\n",
    "    \"sociodem_cycling_experience\": {'1-2': 0, '5-10': 1, '10+': 2},\n",
    "\n",
    "    \"sociodem_weight\": {'<60': 60, '61-70': 65.5, '71-80': 75.5, '81-90': 85.5, '101-110': 105.5},\n",
    "\n",
    "    \"sociodem_length\": {'161-170': 165.5, '171-180': 175.5, '181-190': 185.5, '191-200': 195.5},\n",
    "\n",
    "    \"sociodem_pleasantness_disposition\": {'Rarely': 1, 'Sometimes': 2, 'Often': 3, 'Very often': 4},\n",
    "\n",
    "    \"sociodem_safety_predisposition\": {\"Strongly agree\": 3, \"Agree\": 2, \"Neutral\": 1},\n",
    "\n",
    "    \"sociodem_age\": {'<25': 15, '25-35': 30, \"36-45\": 40, '46-55': 50, '56-65': 60, '66-75': 70, '75<': 75}, \n",
    "\n",
    "    \"sociodem_nervousness_predisposition\": {\"Strongly disagree\": 3, \"Disagree\": 2, \"Neutral\": 1},\n",
    "\n",
    "    \"sociodem_arousal_disposition\": {\"Never\": 1, \"Rarely\": 2, \"Sometimes\": 3, \"Often\": 4, \"Very often\": 5},\n",
    "\n",
    "    \"sociodem_flow_disposition\": {\"Rarely\": 1, \"Sometimes\": 2, \"Often\": 3, \"Very often\": 4},\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "# Select and rename the columns based on the mapping\n",
    "sociodem_subset = sociodem[list(column_name_mapping.keys())]\n",
    "\n",
    "# Rename columns using the column_mapping\n",
    "sociodem_subset.rename(columns=column_name_mapping, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# List of p_ids to skip\n",
    "#skip_p_ids = []  # Add any p_ids you want to skip\n",
    "skip_p_ids = [f\"{i:02}\" for i in range(1)]\n",
    "\n",
    "# Iterate through each subfolder in the root folder\n",
    "for p_id in os.listdir(parent_dir):\n",
    "    folder_path = os.path.join(parent_dir, p_id)\n",
    "    \n",
    "    if p_id in skip_p_ids:\n",
    "        print(f\"Skipping folder as instructed: {folder_path} (p_id {p_id})\")\n",
    "        continue\n",
    "    \n",
    "    if os.path.isdir(folder_path):\n",
    "        \n",
    "        print(f\"Processing folder: {folder_path}\")\n",
    "                \n",
    "        ratings_features = load_file_into_dataframe(folder_path, 'ratingsFeatures_baselcorr_17-3_C1WPCSW', 'csv', ',')\n",
    "\n",
    "        if ratings_features is None or ratings_features.empty:\n",
    "            print(f\"ratings_features missing or empty in folder {folder_path}.\")\n",
    "            continue\n",
    "\n",
    "        ratings_features['timestamp_button'] = pd.to_datetime(ratings_features['timestamp_button'], errors='coerce')\n",
    "        ratings_features['p_id'] = ratings_features['p_id'].astype(str).str.lstrip('0').astype('int64')\n",
    "\n",
    "\n",
    "        results = []\n",
    "        \n",
    "        if ratings_features is not None:\n",
    "\n",
    "            # Merge based on p_id\n",
    "            ratings_features = ratings_features.merge(\n",
    "                sociodem_subset,\n",
    "                left_on='p_id',\n",
    "                right_on='P_ID',\n",
    "                how='left'\n",
    "            )\n",
    "\n",
    "            # Optionally, drop the duplicate 'P_ID' column if not needed\n",
    "            ratings_features.drop(columns=['P_ID'], inplace=True)\n",
    "\n",
    "            # Add context_influence and warning_value based on interval_id\n",
    "            for index, row in ratings_features.iterrows():\n",
    "                if row['interval_id'] == 1:\n",
    "                    ratings_features.at[index, 'sociodem_context_influence'] = row['context_positivity_1']\n",
    "                    ratings_features.at[index, 'sociodem_warning_value'] = row['warning_value_1']\n",
    "                elif row['interval_id'] == 2:\n",
    "                    ratings_features.at[index, 'sociodem_context_influence'] = row['context_positivity_2']\n",
    "                    ratings_features.at[index, 'sociodem_warning_value'] = row['warning_value_2']\n",
    "                elif row['interval_id'] == 3:\n",
    "                    ratings_features.at[index, 'sociodem_context_influence'] = row['context_positivity_3']\n",
    "                    ratings_features.at[index, 'sociodem_warning_value'] = row['warning_value_3']\n",
    "\n",
    "            ratings_features = ratings_features.drop({'context_positivity_1', 'warning_value_1', 'context_positivity_2', 'warning_value_2', 'context_positivity_3', 'warning_value_3'}, axis=1)\n",
    "\n",
    "\n",
    "            # Apply mappings and create new columns with \"_mapped\" suffix\n",
    "            for col, mapping in column_numerical_mapping.items():\n",
    "                ratings_features[col + \"_mapped\"] = ratings_features[col].map(mapping)\n",
    "\n",
    "            # Calculate BMI using the mapped weight and length\n",
    "            ratings_features[\"sociodem_BMI\"] = ratings_features[\"sociodem_weight_mapped\"] / ((ratings_features[\"sociodem_length_mapped\"] / 100) ** 2)\n",
    "\n",
    "\n",
    "            # Save the corrected DataFrame to a new CSV file in the same folder\n",
    "            output_file_path = os.path.join(folder_path, f\"{p_id}_ratingsFeatures_baselcorr_17-3_C1WPCSWS.csv\")\n",
    "            ratings_features.to_csv(output_file_path, index=None)\n",
    "            print(f\"Saved merged features to: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded file: C:\\Users\\BootMR\\Documents\\data_export\\03\\03_ratingsFeatures_baselcorr_17-3_C1WPCSWS.csv, Rows: 8\n",
      "Saved merged DataFrame to: C:\\Users\\BootMR\\Documents\\data_export\\ratingsFeatures_baselcorr_17-3_C1WPCSWS_merged.csv, Total Rows: 8\n"
     ]
    }
   ],
   "source": [
    "# as prep for adding FB data\n",
    "\n",
    "## merge all individual files into 1 large\n",
    "\n",
    "\n",
    "\n",
    "# Initialize a list to store DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Iterate over all subfolders\n",
    "for subdir, _, files in os.walk(parent_dir):\n",
    "    # Skip the root directory itself\n",
    "    if subdir == parent_dir:\n",
    "        continue\n",
    "    \n",
    "    # Extract the subfolder name\n",
    "    subfolder_name = os.path.basename(subdir)\n",
    "    \n",
    "    # Check for files containing \"FlirtNkFeatures\" in the current subfolder\n",
    "    for file in files:\n",
    "        if \"ratingsFeatures_baselcorr_17-3_C1WPCSWS\" in file:\n",
    "            file_path = os.path.join(subdir, file)\n",
    "            try:\n",
    "                # Read the file into a DataFrame\n",
    "                df = pd.read_csv(file_path)\n",
    "                # Append the DataFrame to the list\n",
    "                dataframes.append(df)\n",
    "                print(f\"Loaded file: {file_path}, Rows: {len(df)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file {file_path}: {e}\")\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "if dataframes:\n",
    "    merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "    # Define the root directory and the output file path\n",
    "    output_file_path = os.path.join(parent_dir, \"ratingsFeatures_baselcorr_17-3_C1WPCSWS_merged.csv\")\n",
    "    # Save the merged DataFrame to a CSV file\n",
    "    merged_df.to_csv(output_file_path, index=False)\n",
    "    print(f\"Saved merged DataFrame to: {output_file_path}, Total Rows: {len(merged_df)}\")\n",
    "else:\n",
    "    print(\"No files with 'FlirtNkFeatures' found in any subfolder.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add fietsersbond data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### done in arcgis\n",
    "\n",
    "all_ratingsfeatures = pd.read_csv(\"ratingsFeatures_baselcorr_17-3_C1WPCSWSFB_merged.csv\", sep=\";\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clean after adding FB data\n",
    "\n",
    "#all_ratingsfeatures = all_ratingsfeatures.drop({'Join_Count', 'TARGET_FID'}, axis=1)\n",
    "\n",
    "# List of columns to add the prefix 'FB_'\n",
    "columns_to_prefix = [\n",
    "    'id', 'van_id', 'naar_id', 'lengte', 'toegang', 'straat', 'has_straat', 'wegnummer', \n",
    "    'plaats', 'has_plaats', 'provincie', 'navigatie', 'ongelijkv', 'wegNiveau', 'wegtype', \n",
    "    'wegdeksrt', 'wegkwal', 'hinder', 'verlichtin', 'omgeving', 'water', 'schoonheid', \n",
    "    'beschrvng', 'extreistij', 'maxsnelhei', 'hoofdroute', 'knooppunt', 'lf', 'routes', \n",
    "    'has_routes', 'BiBK', 'wegbeheer', 'strooien', 'snelfiets', 'breedtekls', 'gem_st_fwd', \n",
    "    'gem_st_bwd', 'max_st_fwd', 'max_st_bwd', 'toegang_sp', 'lanes_fwd', 'lanes_bwd', 'is_af'\n",
    "]\n",
    "\n",
    "# Add the prefix 'FB_' to the selected columns in all_ratingsfeatures\n",
    "all_ratingsfeatures.rename(columns={col: f\"FB_{col}\" for col in columns_to_prefix}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Define mapping dictionaries\n",
    "column_FB_numerical_mapping = {\n",
    "    \"FB_wegNiveau\": {\"belangrijke hoofdweg\": 1, \"langs hoofdweg\": 2, \"overige weg\": 3},\n",
    "    \"FB_wegtype\": {\"normale weg\": 1, \"fietspad (langs weg)\": 2, \"ventweg\": 3, \n",
    "                   \"weg met fiets(suggestie)strook\": 4, \"fietsstraat\": 5, \"solitair fietspad\": 6},\n",
    "    \"FB_wegdeksrt\": {\"tegels\": 1, \"klinkers\": 2, \"halfverhard\": 3, \"asfalt/beton\": 4},\n",
    "    \"FB_wegkwal\": {\"goed\": 3, \"redelijk\": 2},\n",
    "    \"FB_hinder\": {\"veel\": 1, \"redelijk\": 2, \"weinig\": 3, \"zeer weinig\": 4},\n",
    "    \"FB_omgeving\": {\"bebouwd (weinig of geen groen)\": 1, \"bebouwd (veel groen)\": 2, \"bos\": 3},\n",
    "    \"FB_schoonheid\": {\"mooi\": 2, \"neutraal\": 1},\n",
    "    \"FB_maxsnelhei\": {\"30\": 1, \"50\": 2}\n",
    "}\n",
    "\n",
    "# Iterate over each column mapping\n",
    "for column, mapping in column_FB_numerical_mapping.items():\n",
    "    # Check if the column exists in the dataframe\n",
    "    if column in all_ratingsfeatures.columns:\n",
    "        # Apply the mapping and create new column with _mapped suffix\n",
    "        mapped_column_name = f\"{column}_mapped\"\n",
    "        all_ratingsfeatures[mapped_column_name] = all_ratingsfeatures[column].map(mapping)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = os.path.join(parent_dir, \"ratingsFeatures_baselcorr_17-3_C1WPCSWSFBC_merged.csv\")\n",
    "# Save the merged DataFrame to a CSV file\n",
    "all_ratingsfeatures.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## renaming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the following provides input for what should be corrected\n",
    "\n",
    "rename_mapping = {\n",
    "    #old:new\n",
    "    \"sociodem_income\": \"sociodem_income_original\",\n",
    "    \"sociodem_education\": \"sociodem_education_original\",\n",
    "    \"sociodem_cycling_experience\": \"timestamp_sociodem_cycling_experience_mappedwindow_original\",\n",
    "    \"sociodem_pleasantness_disposition\": \"sociodem_pleasantness_disposition_original\",\n",
    "\n",
    "    \"sociodem_income_mapped\": \"sociodem_income\",\n",
    "    \"sociodem_education_mapped\": \"sociodem_education\",\n",
    "    \"sociodem_cycling_experience_mapped\": \"sociodem_cycling_experience\",\n",
    "    \"sociodem_pleasantness_disposition_mapped\": \"sociodem_pleasantness_disposition\",\n",
    "\n",
    "    \"sociodem_warning_value\": \"warnings_warning_value\",\n",
    "\n",
    "    \"FB_wegdeksrt_mapped\": \"context_surface_type\",\n",
    "    \"FB_wegkwal_mapped\": \"context_road_quality\",\n",
    "    \"FB_schoonheid_mapped\": \"context_scenic_beauty\",\n",
    "    \"FB_hinder_mapped\": \"context_hindrance\",\n",
    "    \"FB_omgeving_mapped\": \"context_surroundings\",\n",
    "    \"FB_wegtype_mapped\": \"context_road_type\"\n",
    "}\n",
    "all_ratings_BEWSFB = all_ratings_BEWSFB.rename(columns=rename_mapping)\n",
    "\n",
    "\n",
    "'''\n",
    "#then construct a new list for feature_selection and R\n",
    "\n",
    "add: \n",
    "\n",
    "all_ratings_BEWSFB[\"context_perceivedinfluence\"]\n",
    "all_ratings_BEWSFB[\"cycling_perceivedintensity\"] \n",
    "all_ratings_BEWSFB[\"sociodem_fitness\"] \n",
    "all_ratings_BEWSFB[\"sociodem_mood\"] \n",
    "\n",
    "into: \n",
    "\n",
    "p_id + rating_totalperpid + H10_hr_mean + H10_hrv_mean + H10_HRV_RMSSD + H10_HRV_MeanNN + H10_HRV_HF + H10_HRV_SD1 + H10_HRV_SD2 + E4_eda_phasic_mean + E4_eda_phasic_peaks + E4_eda_phasic_max + E4_eda_phasic_n_above_mean + E4_eda_tonic_mean + E4_eda_tonic_peaks + E4_eda_tonic_n_above_mean + cadence_avg + velocity_avg + velocity_avg_change + warnings_slowdown_count + warnings_tactile_warning + warnings_audio_warning + warnings_warning_value + sociodem_income + sociodem_education + sociodem_income + sociodem_education + sociodem_cycling_experience + sociodem_BMI + sociodem_fitness + sociodem_mood + sociodem_pleasantness_disposition + context_surface_type + context_road_quality + context_scenic_beauty + context_hindrance + context_road_type + weather_rain_3h + weather_wind_speed + context_perceivedinfluence + cycling_perceivedintensity\n",
    "\n",
    "'''\n",
    "\n",
    "# Optional: save to new file\n",
    "all_ratings_BEWSFB.to_csv(\"ratingsFeatures_baselcorr_17-3_C1WPCSWSFBCNSR_merged.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

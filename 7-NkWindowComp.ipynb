{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import & config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\BootMR\\Documents\\data_export\n"
     ]
    }
   ],
   "source": [
    "import neurokit2 as nk\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "# Specify the path to the desired directory\n",
    "parent_dir = r'<<< PLACE HERE DIRECTORY WITH DATASET >>>'\n",
    "\n",
    "# Change the current working directory to the specified directory\n",
    "os.chdir(parent_dir)\n",
    "\n",
    "mastertimesheet = pd.read_excel(\"mastertimesheet-4.xlsx\")\n",
    "\n",
    "# Add leading zero to p_id values below 10\n",
    "mastertimesheet['p_id'] = mastertimesheet['p_id'].apply(lambda x: str(x).zfill(2))\n",
    "\n",
    "# Verify that the working directory has been changed\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "# Function to load file into a DataFrame\n",
    "def load_file_into_dataframe(folder_path, var, filetype, sep=','):\n",
    "    var_files = [f for f in os.listdir(folder_path) if f.endswith(filetype) and var in f]\n",
    "    \n",
    "    if var_files:\n",
    "        file_path = os.path.join(folder_path, var_files[0])\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, sep=sep)\n",
    "            print(f\"Loaded file: {file_path}\")\n",
    "            return df\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"The file {file_path} is empty.\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred while reading the file {file_path}: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"No file with '{var}' in its name found in folder {folder_path}.\")\n",
    "        return None\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", module=\"neurokit2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge features and stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: C:\\Users\\BootMR\\Documents\\data_export\\00-code_export\n",
      "No file with '_ecg' in its name found in folder C:\\Users\\BootMR\\Documents\\data_export\\00-code_export.\n",
      "No file with 'buttons_gps.csv' in its name found in folder C:\\Users\\BootMR\\Documents\\data_export\\00-code_export.\n",
      "No file with '_hr' in its name found in folder C:\\Users\\BootMR\\Documents\\data_export\\00-code_export.\n",
      "ECG file is missing or empty in folder C:\\Users\\BootMR\\Documents\\data_export\\00-code_export.\n",
      "Processing folder: C:\\Users\\BootMR\\Documents\\data_export\\03\n",
      "Loaded file: C:\\Users\\BootMR\\Documents\\data_export\\03\\polar_h10_cbd9da26_20240522_111821_ecg.txt\n",
      "Loaded file: C:\\Users\\BootMR\\Documents\\data_export\\03\\03_buttons_gps.csv\n",
      "Loaded file: C:\\Users\\BootMR\\Documents\\data_export\\03\\polar_h10_cbd9da26_20240522_111820_hr.txt\n",
      "Processing time window: 30s for 03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 03 - 30s: 100%|██████████| 8/8 [00:03<00:00,  2.59entry/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BootMR\\Documents\\data_export\\03\\03_ratingsECGfeatures_30s.csv successfully saved with HRV and HR statistics for 03\n",
      "Processing time window: 60s for 03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 03 - 60s: 100%|██████████| 8/8 [00:05<00:00,  1.37entry/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BootMR\\Documents\\data_export\\03\\03_ratingsECGfeatures_60s1.csv successfully saved with HRV and HR statistics for 03\n",
      "Processing time window: 60s for 03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 03 - 60s: 100%|██████████| 8/8 [00:06<00:00,  1.32entry/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BootMR\\Documents\\data_export\\03\\03_ratingsECGfeatures_60s2.csv successfully saved with HRV and HR statistics for 03\n",
      "Processing time window: 120s for 03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 03 - 120s: 100%|██████████| 8/8 [00:13<00:00,  1.70s/entry]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BootMR\\Documents\\data_export\\03\\03_ratingsECGfeatures_120s.csv successfully saved with HRV and HR statistics for 03\n",
      "Processing folder: C:\\Users\\BootMR\\Documents\\data_export\\04\n",
      "No file with '_ecg' in its name found in folder C:\\Users\\BootMR\\Documents\\data_export\\04.\n",
      "Loaded file: C:\\Users\\BootMR\\Documents\\data_export\\04\\04_buttons_gps.csv\n",
      "No file with '_hr' in its name found in folder C:\\Users\\BootMR\\Documents\\data_export\\04.\n",
      "ECG file is missing or empty in folder C:\\Users\\BootMR\\Documents\\data_export\\04.\n",
      "Processing folder: C:\\Users\\BootMR\\Documents\\data_export\\FB\n",
      "No file with '_ecg' in its name found in folder C:\\Users\\BootMR\\Documents\\data_export\\FB.\n",
      "No file with 'buttons_gps.csv' in its name found in folder C:\\Users\\BootMR\\Documents\\data_export\\FB.\n",
      "No file with '_hr' in its name found in folder C:\\Users\\BootMR\\Documents\\data_export\\FB.\n",
      "ECG file is missing or empty in folder C:\\Users\\BootMR\\Documents\\data_export\\FB.\n",
      "Processing complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#### worked well 1st March\n",
    "# all features and descriptive stats\n",
    "\n",
    "\n",
    "# Define window configurations\n",
    "window_configs = [\n",
    "    (30, 15, 15, \"{p_id}_ratingsECGfeatures_30s.csv\"),\n",
    "    (60, 30, 30, \"{p_id}_ratingsECGfeatures_60s1.csv\"),\n",
    "    (60, 20, 40, \"{p_id}_ratingsECGfeatures_60s2.csv\"),\n",
    "    (120, 60, 60, \"{p_id}_ratingsECGfeatures_120s.csv\")\n",
    "]\n",
    "\n",
    "# List of p_ids to skip\n",
    "skip_p_ids = [f\"{i:02}\" for i in range(1)]\n",
    "\n",
    "# Iterate through each participant's folder\n",
    "for p_id in os.listdir(parent_dir):\n",
    "    folder_path = os.path.join(parent_dir, p_id)\n",
    "    \n",
    "    if p_id in skip_p_ids:\n",
    "        print(f\"Skipping folder as instructed: {folder_path} (p_id {p_id})\")\n",
    "        continue\n",
    "    \n",
    "    if os.path.isdir(folder_path):\n",
    "        print(f\"Processing folder: {folder_path}\")\n",
    "        \n",
    "        # Load ECG, buttons, and HR data\n",
    "        ecg = load_file_into_dataframe(folder_path, '_ecg', '.txt', ';')\n",
    "        buttons = load_file_into_dataframe(folder_path, 'buttons_gps.csv', '.csv', ',')\n",
    "        hr = load_file_into_dataframe(folder_path, '_hr', '.txt', ';')\n",
    "\n",
    "        if ecg is None or ecg.empty:\n",
    "            print(f\"ECG file is missing or empty in folder {folder_path}.\")\n",
    "            continue\n",
    "        if buttons is None or buttons.empty:\n",
    "            print(f\"Buttons file is missing or empty in folder {folder_path}.\")\n",
    "            continue\n",
    "        if hr is None or hr.empty:\n",
    "            print(f\"HR file is missing or empty in folder {folder_path}.\")\n",
    "            continue\n",
    "        \n",
    "        # Convert timestamps to datetime\n",
    "        ecg['Phone timestamp'] = pd.to_datetime(ecg['Phone timestamp'])\n",
    "        buttons['timestamp_button'] = pd.to_datetime(buttons['timestamp_button'])\n",
    "        hr['Phone timestamp'] = pd.to_datetime(hr['Phone timestamp'])\n",
    "\n",
    "        # Iterate over different time windows\n",
    "        for window_length, interval_before, interval_after, filename_template in window_configs:\n",
    "            print(f\"Processing time window: {window_length}s for {p_id}\")\n",
    "            button_features = pd.DataFrame()\n",
    "            \n",
    "            with tqdm(total=len(buttons), desc=f\"Processing {p_id} - {window_length}s\", unit=\"entry\") as pbar:\n",
    "                for index, row in buttons.iterrows():\n",
    "                    timestamp = row['timestamp_button']\n",
    "                    rating = row['rating']\n",
    "                    \n",
    "                    start_time = timestamp - pd.Timedelta(seconds=interval_before)\n",
    "                    end_time = timestamp + pd.Timedelta(seconds=interval_after)\n",
    "                    \n",
    "                    # Filter ECG data within the time window\n",
    "                    filtered_ecg = ecg[(ecg['Phone timestamp'] >= start_time) & (ecg['Phone timestamp'] <= end_time)]\n",
    "                    \n",
    "                    if filtered_ecg.empty:\n",
    "                        pbar.update(1)\n",
    "                        continue\n",
    "                    \n",
    "                    # Extract ECG signal and process HRV features\n",
    "                    filtered_ecg_ecgonly = filtered_ecg['ecg [uV]']\n",
    "                    filtered_ecg_ecgonly_signals, _ = nk.ecg_process(filtered_ecg_ecgonly, sampling_rate=130, method='neurokit')\n",
    "                    hrv_features = nk.hrv(filtered_ecg_ecgonly_signals['ECG_R_Peaks'], sampling_rate=130, nperseg=256, show=False)\n",
    "                    \n",
    "                    # Compute HR statistics in the same window\n",
    "                    filtered_hr = hr[(hr['Phone timestamp'] >= start_time) & (hr['Phone timestamp'] <= end_time)]\n",
    "                    \n",
    "                    if not filtered_hr.empty:\n",
    "                        hr_mean = round(filtered_hr['HR [bpm]'].mean(), 2)\n",
    "                        hr_stdev = round(filtered_hr['HR [bpm]'].std(), 2)\n",
    "                        hrv_mean = round(filtered_hr['HRV [ms]'].mean(), 2)\n",
    "                        hrv_stdev = round(filtered_hr['HRV [ms]'].std(), 2)\n",
    "                    else:\n",
    "                        hr_mean, hr_stdev, hrv_mean, hrv_stdev = None, None, None, None\n",
    "                    \n",
    "                    # Add computed features to the dataframe\n",
    "                    hrv_features.insert(0, 'button_timestamp', timestamp)\n",
    "                    hrv_features.insert(1, 'rating', rating)\n",
    "                    hrv_features.insert(2, 'window_start_time', start_time)\n",
    "                    hrv_features.insert(3, 'window_end_time', end_time)\n",
    "                    hrv_features['hr_mean'] = hr_mean\n",
    "                    hrv_features['hr_stdev'] = hr_stdev\n",
    "                    hrv_features['hrv_mean'] = hrv_mean\n",
    "                    hrv_features['hrv_stdev'] = hrv_stdev\n",
    "                    \n",
    "                    # Append to results dataframe\n",
    "                    button_features = pd.concat([button_features, hrv_features], ignore_index=True)\n",
    "                    pbar.update(1)\n",
    "            \n",
    "            # Merge with button press data and save results\n",
    "            merged_features = pd.merge(buttons, button_features, how='left', left_on='timestamp_button', right_on='button_timestamp')\n",
    "            merged_features.drop(columns=['button_timestamp', 'rating_y'], inplace=True)\n",
    "            merged_features.rename(columns={'rating_x': 'rating'}, inplace=True)\n",
    "            merged_features.insert(0, 'rating', merged_features.pop('rating'))\n",
    "\n",
    "            # Insert HR statistics in correct column order\n",
    "            column_order = list(merged_features.columns)\n",
    "            if 'HRV_MeanNN' in column_order:\n",
    "                window_end_time_index = column_order.index('window_end_time')\n",
    "                hrv_mean_nn_index = column_order.index('HRV_MeanNN')\n",
    "\n",
    "                new_column_order = (\n",
    "                    column_order[:window_end_time_index + 1] +\n",
    "                    ['hr_mean', 'hr_stdev', 'hrv_mean', 'hrv_stdev'] +\n",
    "                    column_order[hrv_mean_nn_index:]\n",
    "                )\n",
    "\n",
    "                merged_features = merged_features[new_column_order]\n",
    "            \n",
    "            # Delete the last 4 columns\n",
    "            merged_features = merged_features.iloc[:, :-4]\n",
    "\n",
    "            # Save the results\n",
    "            output_file_path = os.path.join(folder_path, filename_template.format(p_id=p_id))\n",
    "            merged_features.to_csv(output_file_path, index=False)\n",
    "            print(f\"{output_file_path} successfully saved with HRV and HR statistics for {p_id}\")\n",
    "\n",
    "print(\"Processing complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## baseline correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: C:\\Users\\BootMR\\Documents\\data_export\\00-code_export\n",
      "No file with '_baseline_HRVfeatures.csv' in its name found in folder C:\\Users\\BootMR\\Documents\\data_export\\00-code_export.\n",
      "baseline_ECGfeatures missing or empty in folder C:\\Users\\BootMR\\Documents\\data_export\\00-code_export.\n",
      "Processing folder: C:\\Users\\BootMR\\Documents\\data_export\\03\n",
      "Loaded file: C:\\Users\\BootMR\\Documents\\data_export\\03\\03_baseline_HRVfeatures.csv\n",
      "Loaded file: C:\\Users\\BootMR\\Documents\\data_export\\03\\03_ratingsECGfeatures_30s.csv\n",
      "Baseline-corrected file saved: C:\\Users\\BootMR\\Documents\\data_export\\03\\03_ratings_HRV_baselinecorrected_30s.csv for 03\n",
      "Loaded file: C:\\Users\\BootMR\\Documents\\data_export\\03\\03_ratingsECGfeatures_60s1.csv\n",
      "Baseline-corrected file saved: C:\\Users\\BootMR\\Documents\\data_export\\03\\03_ratings_HRV_baselinecorrected_60s1.csv for 03\n",
      "Loaded file: C:\\Users\\BootMR\\Documents\\data_export\\03\\03_ratingsECGfeatures_60s2.csv\n",
      "Baseline-corrected file saved: C:\\Users\\BootMR\\Documents\\data_export\\03\\03_ratings_HRV_baselinecorrected_60s2.csv for 03\n",
      "Loaded file: C:\\Users\\BootMR\\Documents\\data_export\\03\\03_ratingsECGfeatures_120s.csv\n",
      "Baseline-corrected file saved: C:\\Users\\BootMR\\Documents\\data_export\\03\\03_ratings_HRV_baselinecorrected_120s.csv for 03\n",
      "Finished processing folder C:\\Users\\BootMR\\Documents\\data_export\\03\n",
      "Processing folder: C:\\Users\\BootMR\\Documents\\data_export\\04\n",
      "No file with '_baseline_HRVfeatures.csv' in its name found in folder C:\\Users\\BootMR\\Documents\\data_export\\04.\n",
      "baseline_ECGfeatures missing or empty in folder C:\\Users\\BootMR\\Documents\\data_export\\04.\n",
      "Skipping non-directory: C:\\Users\\BootMR\\Documents\\data_export\\all_baselineHRVfeatures.csv\n",
      "Skipping non-directory: C:\\Users\\BootMR\\Documents\\data_export\\all_baseline_FlirtFeatures_means.csv\n",
      "Skipping non-directory: C:\\Users\\BootMR\\Documents\\data_export\\all_ratingsflirtneurokit.csv\n",
      "Skipping non-directory: C:\\Users\\BootMR\\Documents\\data_export\\all_ratingswHRV.csv\n",
      "Skipping non-directory: C:\\Users\\BootMR\\Documents\\data_export\\all_ratingswHRV_corrected.csv\n",
      "Processing folder: C:\\Users\\BootMR\\Documents\\data_export\\FB\n",
      "No file with '_baseline_HRVfeatures.csv' in its name found in folder C:\\Users\\BootMR\\Documents\\data_export\\FB.\n",
      "baseline_ECGfeatures missing or empty in folder C:\\Users\\BootMR\\Documents\\data_export\\FB.\n",
      "Skipping non-directory: C:\\Users\\BootMR\\Documents\\data_export\\mastertimesheet-4.xlsx\n",
      "Skipping non-directory: C:\\Users\\BootMR\\Documents\\data_export\\ratingsFeatures_baselcorr_17-3_C1WPCSWSFBC_merged-EXPORT.csv\n",
      "Skipping non-directory: C:\\Users\\BootMR\\Documents\\data_export\\ratingsFeatures_baselcorr_17-3_C1WPCSWSFBC_merged.csv\n",
      "Skipping non-directory: C:\\Users\\BootMR\\Documents\\data_export\\ratingsFeatures_baselcorr_17-3_C1WPCSWSFB_merged.csv\n",
      "Skipping non-directory: C:\\Users\\BootMR\\Documents\\data_export\\ratingsFeatures_baselcorr_17-3_C1WPCSWS_merged.csv\n",
      "Skipping non-directory: C:\\Users\\BootMR\\Documents\\data_export\\ratingsFeatures_extended.csv\n",
      "Skipping non-directory: C:\\Users\\BootMR\\Documents\\data_export\\README.txt\n",
      "Skipping non-directory: C:\\Users\\BootMR\\Documents\\data_export\\responses-full-cleaned.xlsx\n"
     ]
    }
   ],
   "source": [
    "####### worked well 1st March\n",
    "\n",
    "\n",
    "skip_p_ids = [f\"{i:02}\" for i in range(1)]\n",
    "\n",
    "# Iterate through each subfolder in the root folder\n",
    "for p_id in os.listdir(parent_dir):\n",
    "    folder_path = os.path.join(parent_dir, p_id)\n",
    "    \n",
    "    if p_id in skip_p_ids:\n",
    "        print(f\"Skipping folder as instructed: {folder_path} (p_id {p_id})\")\n",
    "        continue\n",
    "    \n",
    "    if os.path.isdir(folder_path):\n",
    "        \n",
    "        print(f\"Processing folder: {folder_path}\")\n",
    "        \n",
    "        # Load the baseline ECG features file\n",
    "        baseline_ECGfeatures = load_file_into_dataframe(folder_path, '_baseline_HRVfeatures.csv', '.csv', ',')      \n",
    "\n",
    "        if baseline_ECGfeatures is None or baseline_ECGfeatures.empty:\n",
    "            print(f\"baseline_ECGfeatures missing or empty in folder {folder_path}.\")\n",
    "            continue\n",
    "        \n",
    "        # List of 4 feature files for the current folder\n",
    "        window_configs = [\n",
    "            (30, f\"{p_id}_ratingsECGfeatures_30s.csv\"),\n",
    "            (60, f\"{p_id}_ratingsECGfeatures_60s1.csv\"),\n",
    "            (60, f\"{p_id}_ratingsECGfeatures_60s2.csv\"),\n",
    "            (120, f\"{p_id}_ratingsECGfeatures_120s.csv\")\n",
    "        ]\n",
    "        \n",
    "        for window_length, filename in window_configs:\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            \n",
    "            # Load the ratings ECG features for the current file\n",
    "            ratings_wHRV = load_file_into_dataframe(folder_path, filename, '.csv', ',')\n",
    "\n",
    "            if ratings_wHRV is None or ratings_wHRV.empty:\n",
    "                print(f\"{filename} missing or empty in folder {folder_path}.\")\n",
    "                continue\n",
    "            \n",
    "            # Convert 'timestamp_button' column to datetime\n",
    "            ratings_wHRV['timestamp_button'] = pd.to_datetime(ratings_wHRV['timestamp_button'])\n",
    "\n",
    "            # Ensure all columns in baseline_ECGfeatures exist in ratings_wHRV\n",
    "            for col in baseline_ECGfeatures.columns:\n",
    "                if col not in ratings_wHRV.columns:\n",
    "                    ratings_wHRV[col] = float('nan')  # Fill missing columns with NaN\n",
    "\n",
    "            # Select only the HRV feature columns (excluding non-HRV columns)\n",
    "            hrv_columns = baseline_ECGfeatures.columns\n",
    "\n",
    "            # Subtract baseline features from button ECG features for baseline correction\n",
    "            ratings_wHRV[hrv_columns] = ratings_wHRV[hrv_columns] - baseline_ECGfeatures.iloc[0]\n",
    "\n",
    "            # Optionally, Z-standardize each HRV feature (uncomment if needed)\n",
    "            # ratings_wHRV[hrv_columns] = (ratings_wHRV[hrv_columns] - ratings_wHRV[hrv_columns].mean()) / ratings_wHRV[hrv_columns].std()\n",
    "\n",
    "            # Construct the output filename based on the format you want\n",
    "            if window_length == 60:\n",
    "                # Check if it's the first or second 60s file and set the suffix accordingly\n",
    "                if \"60s1\" in filename:\n",
    "                    suffix = \"60s1\"\n",
    "                else:\n",
    "                    suffix = \"60s2\"\n",
    "            else:\n",
    "                # For 30s and 120s, use window_length directly\n",
    "                suffix = f\"{window_length}s\"\n",
    "\n",
    "            output_file_path = os.path.join(folder_path, f\"{p_id}_ratings_HRV_baselinecorrected_{suffix}.csv\")\n",
    "            \n",
    "            # Save the baseline-corrected DataFrame to CSV for the current feature file\n",
    "            ratings_wHRV.to_csv(output_file_path, index=False)\n",
    "            print(f\"Baseline-corrected file saved: {output_file_path} for {p_id}\")\n",
    "            \n",
    "        print(f\"Finished processing folder {folder_path}\")\n",
    "    else:\n",
    "        print(f\"Skipping non-directory: {folder_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: C:\\Users\\BootMR\\Documents\\data_export\\00-code_export\n",
      "File C:\\Users\\BootMR\\Documents\\data_export\\00-code_export\\00-code_export_ratingsECGfeatures_30s.csv does not exist.\n",
      "File C:\\Users\\BootMR\\Documents\\data_export\\00-code_export\\00-code_export_ratingsECGfeatures_60s1.csv does not exist.\n",
      "File C:\\Users\\BootMR\\Documents\\data_export\\00-code_export\\00-code_export_ratingsECGfeatures_60s2.csv does not exist.\n",
      "File C:\\Users\\BootMR\\Documents\\data_export\\00-code_export\\00-code_export_ratingsECGfeatures_120s.csv does not exist.\n",
      "Processing folder: C:\\Users\\BootMR\\Documents\\data_export\\03\n",
      "Added C:\\Users\\BootMR\\Documents\\data_export\\03\\03_ratingsECGfeatures_30s.csv to merge group '30s'\n",
      "Added C:\\Users\\BootMR\\Documents\\data_export\\03\\03_ratingsECGfeatures_60s1.csv to merge group '60s1'\n",
      "Added C:\\Users\\BootMR\\Documents\\data_export\\03\\03_ratingsECGfeatures_60s2.csv to merge group '60s2'\n",
      "Added C:\\Users\\BootMR\\Documents\\data_export\\03\\03_ratingsECGfeatures_120s.csv to merge group '120s'\n",
      "Processing folder: C:\\Users\\BootMR\\Documents\\data_export\\04\n",
      "File C:\\Users\\BootMR\\Documents\\data_export\\04\\04_ratingsECGfeatures_30s.csv does not exist.\n",
      "File C:\\Users\\BootMR\\Documents\\data_export\\04\\04_ratingsECGfeatures_60s1.csv does not exist.\n",
      "File C:\\Users\\BootMR\\Documents\\data_export\\04\\04_ratingsECGfeatures_60s2.csv does not exist.\n",
      "File C:\\Users\\BootMR\\Documents\\data_export\\04\\04_ratingsECGfeatures_120s.csv does not exist.\n",
      "Processing folder: C:\\Users\\BootMR\\Documents\\data_export\\FB\n",
      "File C:\\Users\\BootMR\\Documents\\data_export\\FB\\FB_ratingsECGfeatures_30s.csv does not exist.\n",
      "File C:\\Users\\BootMR\\Documents\\data_export\\FB\\FB_ratingsECGfeatures_60s1.csv does not exist.\n",
      "File C:\\Users\\BootMR\\Documents\\data_export\\FB\\FB_ratingsECGfeatures_60s2.csv does not exist.\n",
      "File C:\\Users\\BootMR\\Documents\\data_export\\FB\\FB_ratingsECGfeatures_120s.csv does not exist.\n",
      "Merged file saved: C:\\Users\\BootMR\\Documents\\data_export\\merged_30s.csv\n",
      "Merged file saved: C:\\Users\\BootMR\\Documents\\data_export\\merged_60s1.csv\n",
      "Merged file saved: C:\\Users\\BootMR\\Documents\\data_export\\merged_60s2.csv\n",
      "Merged file saved: C:\\Users\\BootMR\\Documents\\data_export\\merged_120s.csv\n"
     ]
    }
   ],
   "source": [
    "####### merge non-baselinecorrected\n",
    "\n",
    "\n",
    "# Suffixes to group files by\n",
    "suffixes = ['30s', '60s1', '60s2', '120s']\n",
    "\n",
    "# Initialize a dictionary to hold dataframes for each suffix\n",
    "merged_data = {suffix: [] for suffix in suffixes}\n",
    "\n",
    "# Iterate through each subfolder in the root folder\n",
    "for p_id in os.listdir(parent_dir):\n",
    "    folder_path = os.path.join(parent_dir, p_id)\n",
    "    \n",
    "    if os.path.isdir(folder_path):\n",
    "        print(f\"Processing folder: {folder_path}\")\n",
    "        \n",
    "        # Check each suffix group\n",
    "        for suffix in suffixes:\n",
    "            # Construct the filename pattern for the current suffix\n",
    "            filename = f\"{p_id}_ratingsECGfeatures_{suffix}.csv\"\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            \n",
    "            # If the file exists, read it and append to the corresponding list\n",
    "            if os.path.exists(file_path):\n",
    "                df = pd.read_csv(file_path)\n",
    "                df.insert(0, 'p_id', p_id)  # Add subfolder name as the first column\n",
    "                merged_data[suffix].append(df)\n",
    "                print(f\"Added {file_path} to merge group '{suffix}'\")\n",
    "            else:\n",
    "                print(f\"File {file_path} does not exist.\")\n",
    "\n",
    "# Merge the dataframes for each suffix and save to a new CSV file\n",
    "for suffix, dfs in merged_data.items():\n",
    "    if dfs:\n",
    "        # Concatenate all dataframes in the list\n",
    "        merged_df = pd.concat(dfs, ignore_index=True)\n",
    "        \n",
    "        # Save the merged dataframe to a new CSV file\n",
    "        output_file_path = os.path.join(parent_dir, f\"merged_{suffix}.csv\")\n",
    "        merged_df.to_csv(output_file_path, index=False)\n",
    "        print(f\"Merged file saved: {output_file_path}\")\n",
    "    else:\n",
    "        print(f\"No files to merge for suffix '{suffix}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: C:\\Users\\BootMR\\Documents\\data_export\\00-code_export\n",
      "File C:\\Users\\BootMR\\Documents\\data_export\\00-code_export\\00-code_export_ratings_HRV_baselinecorrected_30s.csv does not exist.\n",
      "File C:\\Users\\BootMR\\Documents\\data_export\\00-code_export\\00-code_export_ratings_HRV_baselinecorrected_60s1.csv does not exist.\n",
      "File C:\\Users\\BootMR\\Documents\\data_export\\00-code_export\\00-code_export_ratings_HRV_baselinecorrected_60s2.csv does not exist.\n",
      "File C:\\Users\\BootMR\\Documents\\data_export\\00-code_export\\00-code_export_ratings_HRV_baselinecorrected_120s.csv does not exist.\n",
      "Processing folder: C:\\Users\\BootMR\\Documents\\data_export\\03\n",
      "Added C:\\Users\\BootMR\\Documents\\data_export\\03\\03_ratings_HRV_baselinecorrected_30s.csv to merge group '30s'\n",
      "Added C:\\Users\\BootMR\\Documents\\data_export\\03\\03_ratings_HRV_baselinecorrected_60s1.csv to merge group '60s1'\n",
      "Added C:\\Users\\BootMR\\Documents\\data_export\\03\\03_ratings_HRV_baselinecorrected_60s2.csv to merge group '60s2'\n",
      "Added C:\\Users\\BootMR\\Documents\\data_export\\03\\03_ratings_HRV_baselinecorrected_120s.csv to merge group '120s'\n",
      "Processing folder: C:\\Users\\BootMR\\Documents\\data_export\\04\n",
      "File C:\\Users\\BootMR\\Documents\\data_export\\04\\04_ratings_HRV_baselinecorrected_30s.csv does not exist.\n",
      "File C:\\Users\\BootMR\\Documents\\data_export\\04\\04_ratings_HRV_baselinecorrected_60s1.csv does not exist.\n",
      "File C:\\Users\\BootMR\\Documents\\data_export\\04\\04_ratings_HRV_baselinecorrected_60s2.csv does not exist.\n",
      "File C:\\Users\\BootMR\\Documents\\data_export\\04\\04_ratings_HRV_baselinecorrected_120s.csv does not exist.\n",
      "Processing folder: C:\\Users\\BootMR\\Documents\\data_export\\FB\n",
      "File C:\\Users\\BootMR\\Documents\\data_export\\FB\\FB_ratings_HRV_baselinecorrected_30s.csv does not exist.\n",
      "File C:\\Users\\BootMR\\Documents\\data_export\\FB\\FB_ratings_HRV_baselinecorrected_60s1.csv does not exist.\n",
      "File C:\\Users\\BootMR\\Documents\\data_export\\FB\\FB_ratings_HRV_baselinecorrected_60s2.csv does not exist.\n",
      "File C:\\Users\\BootMR\\Documents\\data_export\\FB\\FB_ratings_HRV_baselinecorrected_120s.csv does not exist.\n",
      "Merged file saved: C:\\Users\\BootMR\\Documents\\data_export\\merged_ratings_HRV_baselinecorrected_30s.csv\n",
      "Merged file saved: C:\\Users\\BootMR\\Documents\\data_export\\merged_ratings_HRV_baselinecorrected_60s1.csv\n",
      "Merged file saved: C:\\Users\\BootMR\\Documents\\data_export\\merged_ratings_HRV_baselinecorrected_60s2.csv\n",
      "Merged file saved: C:\\Users\\BootMR\\Documents\\data_export\\merged_ratings_HRV_baselinecorrected_120s.csv\n"
     ]
    }
   ],
   "source": [
    "######## merge baselinecorrected\n",
    "\n",
    "# Initialize a dictionary to hold dataframes for each suffix\n",
    "merged_data = {suffix: [] for suffix in suffixes}\n",
    "\n",
    "# Iterate through each subfolder in the root folder\n",
    "for p_id in os.listdir(parent_dir):\n",
    "    folder_path = os.path.join(parent_dir, p_id)\n",
    "    \n",
    "    if os.path.isdir(folder_path):\n",
    "        print(f\"Processing folder: {folder_path}\")\n",
    "        \n",
    "        # Check each suffix group\n",
    "        for suffix in suffixes:\n",
    "            # Construct the filename pattern for the current suffix\n",
    "            filename = f\"{p_id}_ratings_HRV_baselinecorrected_{suffix}.csv\"\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            \n",
    "            # If the file exists, read it and append to the corresponding list\n",
    "            if os.path.exists(file_path):\n",
    "                df = pd.read_csv(file_path)\n",
    "                merged_data[suffix].append(df)\n",
    "                print(f\"Added {file_path} to merge group '{suffix}'\")\n",
    "            else:\n",
    "                print(f\"File {file_path} does not exist.\")\n",
    "\n",
    "# Merge the dataframes for each suffix and save to a new CSV file\n",
    "for suffix, dfs in merged_data.items():\n",
    "    if dfs:\n",
    "        # Concatenate all dataframes in the list\n",
    "        merged_df = pd.concat(dfs, ignore_index=True)\n",
    "        \n",
    "        # Save the merged dataframe to a new CSV file\n",
    "        output_file_path = os.path.join(parent_dir, f\"merged_ratings_HRV_baselinecorrected_{suffix}.csv\")\n",
    "        merged_df.to_csv(output_file_path, index=False)\n",
    "        print(f\"Merged file saved: {output_file_path}\")\n",
    "    else:\n",
    "        print(f\"No files to merge for suffix '{suffix}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking file: C:\\Users\\BootMR\\Documents\\data_processed\\CHECKIFYOUREALLYNEEDTHISFOLDER\\merged_30s.csv\n",
      "✅ All required features are present.\n",
      "⚠️ NaN values found:\n",
      "HRV_LF      918\n",
      "HRV_LFHF    918\n",
      "dtype: int64\n",
      "\n",
      "Checking file: C:\\Users\\BootMR\\Documents\\data_processed\\CHECKIFYOUREALLYNEEDTHISFOLDER\\merged_60s1.csv\n",
      "✅ All required features are present.\n",
      "✅ No NaN values in any HRV feature.\n",
      "\n",
      "Checking file: C:\\Users\\BootMR\\Documents\\data_processed\\CHECKIFYOUREALLYNEEDTHISFOLDER\\merged_60s2.csv\n",
      "✅ All required features are present.\n",
      "✅ No NaN values in any HRV feature.\n",
      "\n",
      "Checking file: C:\\Users\\BootMR\\Documents\\data_processed\\CHECKIFYOUREALLYNEEDTHISFOLDER\\merged_120s.csv\n",
      "✅ All required features are present.\n",
      "✅ No NaN values in any HRV feature.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# File paths for the merged datasets\n",
    "merged_files = [\n",
    "    r\"C:\\Users\\BootMR\\Documents\\data_processed\\CHECKIFYOUREALLYNEEDTHISFOLDER\\merged_30s.csv\",\n",
    "    r\"C:\\Users\\BootMR\\Documents\\data_processed\\CHECKIFYOUREALLYNEEDTHISFOLDER\\merged_60s1.csv\",\n",
    "    r\"C:\\Users\\BootMR\\Documents\\data_processed\\CHECKIFYOUREALLYNEEDTHISFOLDER\\merged_60s2.csv\",\n",
    "    r\"C:\\Users\\BootMR\\Documents\\data_processed\\CHECKIFYOUREALLYNEEDTHISFOLDER\\merged_120s.csv\"\n",
    "]\n",
    "\n",
    "# Feature subset for classification\n",
    "hrv_features = [\n",
    "    'hr_mean', 'hr_stdev', 'hrv_mean', 'hrv_stdev',\n",
    "    'HRV_MeanNN', 'HRV_RMSSD', 'HRV_LF', 'HRV_HF', 'HRV_LFHF'\n",
    "]\n",
    "\n",
    "# Iterate over each file\n",
    "for file in merged_files:\n",
    "    print(f\"\\nChecking file: {file}\")\n",
    "\n",
    "    try:\n",
    "        # Load CSV file\n",
    "        data = pd.read_csv(file)\n",
    "\n",
    "        # Check if all required features are present\n",
    "        missing_features = [feature for feature in hrv_features if feature not in data.columns]\n",
    "        if missing_features:\n",
    "            print(f\"❌ Missing columns: {missing_features}\")\n",
    "        else:\n",
    "            print(\"✅ All required features are present.\")\n",
    "\n",
    "            # Check for NaN values in required features\n",
    "            nan_counts = data[hrv_features].isna().sum()\n",
    "            total_missing = nan_counts.sum()\n",
    "\n",
    "            if total_missing == 0:\n",
    "                print(\"✅ No NaN values in any HRV feature.\")\n",
    "            else:\n",
    "                print(f\"⚠️ NaN values found:\")\n",
    "                print(nan_counts[nan_counts > 0])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file: C:\\Users\\BootMR\\Documents\\data_processed\\CHECKIFYOUREALLYNEEDTHISFOLDER\\merged_30s.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BootMR\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['HRV_LF' 'HRV_LFHF']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file: C:\\Users\\BootMR\\Documents\\data_processed\\CHECKIFYOUREALLYNEEDTHISFOLDER\\merged_60s1.csv\n",
      "\n",
      "Processing file: C:\\Users\\BootMR\\Documents\\data_processed\\CHECKIFYOUREALLYNEEDTHISFOLDER\\merged_60s2.csv\n",
      "\n",
      "Processing file: C:\\Users\\BootMR\\Documents\\data_processed\\CHECKIFYOUREALLYNEEDTHISFOLDER\\merged_120s.csv\n",
      "\n",
      "--- F1 Score Comparison ---\n",
      "Model                                    Logistic Regression  Random Forest  \\\n",
      "Metric   Classification Dataset                                               \n",
      "F1 Score 3-Class        merged_120s.csv             0.291943       0.574473   \n",
      "                        merged_30s.csv              0.291943       0.538075   \n",
      "                        merged_60s1.csv             0.291943       0.534851   \n",
      "                        merged_60s2.csv             0.288289       0.530684   \n",
      "         Binary         merged_120s.csv             0.578424       0.681755   \n",
      "                        merged_30s.csv              0.578424       0.681998   \n",
      "                        merged_60s1.csv             0.578424       0.728412   \n",
      "                        merged_60s2.csv             0.574791       0.732874   \n",
      "\n",
      "Model                                    SVM (RBF)  \n",
      "Metric   Classification Dataset                     \n",
      "F1 Score 3-Class        merged_120s.csv   0.291943  \n",
      "                        merged_30s.csv    0.291943  \n",
      "                        merged_60s1.csv   0.313049  \n",
      "                        merged_60s2.csv   0.303697  \n",
      "         Binary         merged_120s.csv   0.578424  \n",
      "                        merged_30s.csv    0.578424  \n",
      "                        merged_60s1.csv   0.578424  \n",
      "                        merged_60s2.csv   0.578424  \n",
      "\n",
      "✅ Results saved to 'f1_score_comparison.csv'\n"
     ]
    }
   ],
   "source": [
    "#### worked well 1st of march\n",
    "# if i remember well, the merged files contain non-baselinecorrected values\n",
    "\n",
    "\n",
    "# File paths\n",
    "merged_files = [\n",
    "    r\"C:\\Users\\BootMR\\Documents\\data_processed\\CHECKIFYOUREALLYNEEDTHISFOLDER\\merged_30s.csv\",\n",
    "    r\"C:\\Users\\BootMR\\Documents\\data_processed\\CHECKIFYOUREALLYNEEDTHISFOLDER\\merged_60s1.csv\",\n",
    "    r\"C:\\Users\\BootMR\\Documents\\data_processed\\CHECKIFYOUREALLYNEEDTHISFOLDER\\merged_60s2.csv\",\n",
    "    r\"C:\\Users\\BootMR\\Documents\\data_processed\\CHECKIFYOUREALLYNEEDTHISFOLDER\\merged_120s.csv\"\n",
    "]\n",
    "\n",
    "# Features & target\n",
    "hrv_features = [\n",
    "    'hr_mean', 'hr_stdev', 'hrv_mean', 'hrv_stdev',\n",
    "    'HRV_MeanNN', 'HRV_RMSSD', 'HRV_LF', 'HRV_HF', 'HRV_LFHF'\n",
    "]\n",
    "target_column = \"rating\"  # Target column for classification (-1, 0, 1)\n",
    "\n",
    "# Classifiers to evaluate\n",
    "classifiers = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"SVM (RBF)\": SVC(),\n",
    "}\n",
    "\n",
    "# Store results in a list\n",
    "results = []\n",
    "\n",
    "for file in merged_files:\n",
    "    print(f\"\\nProcessing file: {file}\")\n",
    "\n",
    "    try:\n",
    "        # Load data\n",
    "        data = pd.read_csv(file)\n",
    "\n",
    "        # Ensure required features exist\n",
    "        missing_features = [feature for feature in hrv_features if feature not in data.columns]\n",
    "        if missing_features:\n",
    "            print(f\"❌ Skipping {file} - Missing columns: {missing_features}\")\n",
    "            continue\n",
    "\n",
    "        # Check if target column exists\n",
    "        if target_column not in data.columns:\n",
    "            print(f\"❌ Skipping {file} - Target column '{target_column}' not found\")\n",
    "            continue\n",
    "\n",
    "        # Extract features & target\n",
    "        X = data[hrv_features]\n",
    "        y = data[target_column]\n",
    "\n",
    "        # Handle NaNs by imputing with mean\n",
    "        imputer = SimpleImputer(strategy=\"mean\")\n",
    "        X = imputer.fit_transform(X)\n",
    "\n",
    "        # Standardize the features\n",
    "        scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "\n",
    "        # Store file name\n",
    "        dataset_name = file.split(\"\\\\\")[-1]  # Extract just the filename\n",
    "\n",
    "        ### 3-Way Classification (-1, 0, 1) ###\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        for clf_name, clf in classifiers.items():\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred = clf.predict(X_test)\n",
    "            f1 = f1_score(y_test, y_pred, average=\"weighted\")  # Weighted to account for class imbalance\n",
    "            results.append([\"F1 Score\", dataset_name, \"3-Class\", clf_name, f1])\n",
    "\n",
    "        ### Binary Classification (-1 vs. 1) ###\n",
    "        binary_mask = y != 0  # Remove class 0\n",
    "        X_binary = X[binary_mask]\n",
    "        y_binary = y[binary_mask]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_binary, y_binary, test_size=0.2, random_state=42)\n",
    "\n",
    "        for clf_name, clf in classifiers.items():\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred = clf.predict(X_test)\n",
    "            f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "            results.append([\"F1 Score\", dataset_name, \"Binary\", clf_name, f1])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {file}: {e}\")\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results, columns=[\"Metric\", \"Dataset\", \"Classification\", \"Model\", \"F1 Score\"])\n",
    "\n",
    "# Pivot table for easy readability\n",
    "results_pivot = results_df.pivot(index=[\"Metric\", \"Classification\", \"Dataset\"], columns=\"Model\", values=\"F1 Score\")\n",
    "\n",
    "# Display results in table format\n",
    "print(\"\\n--- F1 Score Comparison ---\")\n",
    "print(results_pivot)\n",
    "\n",
    "# Save to CSV\n",
    "results_pivot.to_csv(\"f1_score_comparison.csv\")\n",
    "print(\"\\n✅ Results saved to 'f1_score_comparison.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\BootMR\\Documents\\data_export\n"
     ]
    }
   ],
   "source": [
    "import neurokit2 as nk\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import xgboost as xgb\n",
    "import shap\n",
    "import scipy.stats as stats\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "from scipy.stats import linregress, ttest_rel, wilcoxon, zscore\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Specify the path to the desired directory\n",
    "parent_dir = r'<<< PLACE HERE DIRECTORY WITH DATASET >>>'\n",
    "\n",
    "# Change the current working directory to the specified directory\n",
    "os.chdir(parent_dir)\n",
    "\n",
    "# Function to load file into a DataFrame\n",
    "def load_file_into_dataframe(folder_path, var, filetype, sep=','):\n",
    "    var_files = [f for f in os.listdir(folder_path) if f.endswith(filetype) and var in f]\n",
    "    \n",
    "    if var_files:\n",
    "        file_path = os.path.join(folder_path, var_files[0])\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, sep=sep)\n",
    "            print(f\"Loaded file: {file_path}\")\n",
    "            return df\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"The file {file_path} is empty.\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred while reading the file {file_path}: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"No file with '{var}' in its name found in folder {folder_path}.\")\n",
    "        return None\n",
    "\n",
    "# Verify that the working directory has been changed\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### code to show all prints from cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge NeuroKit and Flirt features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: 00-code_export\n",
      "No file with 'ratings_HRV_baselinecorrected' in its name found in folder C:\\Users\\BootMR\\Documents\\data_export\\00-code_export.\n",
      "No file with 'flirtFeatures_30s_baselinecorrected' in its name found in folder C:\\Users\\BootMR\\Documents\\data_export\\00-code_export.\n",
      "ratings_HRV_baselinecorrected is missing or empty in folder C:\\Users\\BootMR\\Documents\\data_export\\00-code_export.\n",
      "Processing folder: 03\n",
      "Loaded file: C:\\Users\\BootMR\\Documents\\data_export\\03\\03_ratings_HRV_baselinecorrected.csv\n",
      "Loaded file: C:\\Users\\BootMR\\Documents\\data_export\\03\\03_flirtFeatures_30s_baselinecorrected.csv\n",
      "Saved merged features to: C:\\Users\\BootMR\\Documents\\data_export\\03\\03_ratingsFeatures_baselcorr_17-3.csv\n",
      "Processing folder: 04\n",
      "No file with 'ratings_HRV_baselinecorrected' in its name found in folder C:\\Users\\BootMR\\Documents\\data_export\\04.\n",
      "Loaded file: C:\\Users\\BootMR\\Documents\\data_export\\04\\04_flirtFeatures_30s_baselinecorrected.csv\n",
      "ratings_HRV_baselinecorrected is missing or empty in folder C:\\Users\\BootMR\\Documents\\data_export\\04.\n"
     ]
    }
   ],
   "source": [
    "########### adapted to new filenames\n",
    "## started 3rd Feb, using script that worked on at 28th Jan\n",
    "## this seems to be the correct script for merging buttons with flirt and nk features\n",
    "#MB15-3: worked well when using again to merge and retain all features\n",
    "\n",
    "''' EXPLANATION\n",
    "\n",
    "This script merges data from two different feature sets (likely ECG-related and button press-related data) by aligning their timestamps, cleaning the data, and then saving the merged result into a new file. Here's a step-by-step explanation:\n",
    "1. Setup\n",
    "\n",
    "    Root directory: The script starts by defining a folder path where participant data is stored.\n",
    "    Skip list: There's an empty list (skip_p_ids) to specify which participants should be skipped during processing.\n",
    "\n",
    "2. Function to load files\n",
    "\n",
    "    The function load_file_into_dataframe searches for a file that matches a given keyword (var) and file type (filetype).\n",
    "    If a matching file is found, it is read into a Pandas DataFrame and returned. If the file is missing or empty, the function returns None and prints a message.\n",
    "\n",
    "3. Timestamp cleaning functions\n",
    "\n",
    "    remove_milliseconds: This function removes the milliseconds from a timestamp column and formats it to YYYY-MM-DD HH:MM:SS.\n",
    "    remove_timezone: This function removes the timezone information from the timestamp column.\n",
    "\n",
    "4. Processing each subfolder\n",
    "\n",
    "    The script uses os.walk to go through each participant folder (subfolder).\n",
    "    For each subfolder:\n",
    "        It checks if the folder should be skipped (if it's in the skip_p_ids list).\n",
    "        It looks for two specific files in the folder:\n",
    "            withECGfeatures.csv (button press features with ECG).\n",
    "            features_30s.csv (feature data from the FLIRT package).\n",
    "\n",
    "5. Data Cleaning and Merging\n",
    "\n",
    "    If both files are found:\n",
    "        The script converts timestamps in both files into datetime format to ensure they can be matched.\n",
    "        It shifts the timestamps in the flirt_features file by 2 hours to align them properly with the buttons_withnkfeatures file.\n",
    "        It cleans the timestamps using the earlier helper functions (remove_milliseconds and remove_timezone).\n",
    "\n",
    "6. Merging the DataFrames\n",
    "\n",
    "    The two data files are merged based on the timestamp column.\n",
    "    The merged result:\n",
    "        Moves the 'rating' column to be the first column of the DataFrame.\n",
    "        Reorganizes the 'timestamp' column to sit between the 'window_start_time' and 'window_end_time' columns.\n",
    "\n",
    "7. Save the Merged Data\n",
    "\n",
    "    The final merged DataFrame is saved as a new CSV file in the same participant folder with the naming convention p_id_buttonsFlirtNkFeatures.csv.\n",
    "\n",
    "In Simple Terms:\n",
    "\n",
    "    The script goes through each participant folder.\n",
    "    For each participant, it looks for two specific data files (ECG/button features and FLIRT features).\n",
    "    It aligns the timestamps, cleans them, merges the two datasets, and reorganizes some columns.\n",
    "    Finally, it saves the merged data into a new file for each participant.\n",
    "\n",
    "This ensures the two different data sources are synchronized and combined into a single dataset for analysis.\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "## merge features from both packages\n",
    "\n",
    "# List of p_ids to skip\n",
    "#skip_p_ids = []  # Add any p_ids you want to skip\n",
    "skip_p_ids = [f\"{i:02}\" for i in range(1)] # skip range\n",
    "\n",
    "### load file function deleted\n",
    "\n",
    "# Functions to clean timestamps\n",
    "def remove_milliseconds(df, column_name):\n",
    "    df[column_name] = pd.to_datetime(df[column_name]).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    df[column_name] = pd.to_datetime(df[column_name])\n",
    "\n",
    "def remove_timezone(df, column_name):\n",
    "    df[column_name] = pd.to_datetime(df[column_name]).dt.tz_localize(None)\n",
    "\n",
    "# Iterate through each subfolder in the root folder\n",
    "for subdir, _, files in os.walk(parent_dir):\n",
    "    if subdir == parent_dir:\n",
    "        continue\n",
    "\n",
    "    p_id = os.path.basename(subdir)\n",
    "    \n",
    "    if p_id in skip_p_ids:\n",
    "        print(f\"Skipping folder as instructed: {subdir} (p_id {p_id})\")\n",
    "        continue\n",
    " \n",
    "    print(f\"Processing folder: {p_id}\")\n",
    "\n",
    "    # Load ECG and buttons data\n",
    "    ratings_HRV_baselinecorrected = load_file_into_dataframe(subdir, 'ratings_HRV_baselinecorrected', '.csv', ',')\n",
    "    flirtFeatures_30s_baselinecorrected = load_file_into_dataframe(subdir, 'flirtFeatures_30s_baselinecorrected', '.csv', ',')\n",
    "\n",
    "    \n",
    "\n",
    "    if ratings_HRV_baselinecorrected is None or ratings_HRV_baselinecorrected.empty:\n",
    "        print(f\"ratings_HRV_baselinecorrected is missing or empty in folder {subdir}.\")\n",
    "        continue\n",
    "    \n",
    "    if flirtFeatures_30s_baselinecorrected is None or flirtFeatures_30s_baselinecorrected.empty:\n",
    "        print(f\"flirtFeatures_30s_baselinecorrected is missing or empty in folder {subdir}.\")\n",
    "        continue\n",
    "    \n",
    "    #correct and checkt imestamps\n",
    "    flirtFeatures_30s_baselinecorrected.rename(columns={'timestamp': 'timestamp_flirt', 'oldName2': 'newName2'}, inplace=True)\n",
    "    flirtFeatures_30s_baselinecorrected['timestamp_flirt'] = pd.to_datetime(flirtFeatures_30s_baselinecorrected['timestamp_flirt']) + pd.Timedelta(hours=2)\n",
    "    ratings_HRV_baselinecorrected['timestamp_button'] = pd.to_datetime(ratings_HRV_baselinecorrected['timestamp_button'])\n",
    "\n",
    "    # Check and create window_start_time if missing\n",
    "    if 'window_start_time' not in ratings_HRV_baselinecorrected.columns:\n",
    "        ratings_HRV_baselinecorrected['window_start_time'] = ratings_HRV_baselinecorrected['timestamp_button'] - pd.Timedelta(seconds=15)\n",
    "    else:\n",
    "        ratings_HRV_baselinecorrected['window_start_time'] = pd.to_datetime(ratings_HRV_baselinecorrected['window_start_time'])\n",
    "\n",
    "    # Check and create window_end_time if missing\n",
    "    if 'window_end_time' not in ratings_HRV_baselinecorrected.columns:\n",
    "        ratings_HRV_baselinecorrected['window_end_time'] = ratings_HRV_baselinecorrected['timestamp_button'] + pd.Timedelta(seconds=15)\n",
    "    else:\n",
    "        ratings_HRV_baselinecorrected['window_end_time'] = pd.to_datetime(ratings_HRV_baselinecorrected['window_end_time'])\n",
    "\n",
    "\n",
    "    ### start merge\n",
    "\n",
    " \n",
    "    if ratings_HRV_baselinecorrected is not None and flirtFeatures_30s_baselinecorrected is not None:\n",
    "\n",
    "        merged_rows = []  # List to store the merged rows\n",
    "        merged_features = [] #initialize empty\n",
    "\n",
    "        for idx, button_row in ratings_HRV_baselinecorrected.iterrows():\n",
    "            timestamp = button_row['window_start_time']\n",
    "\n",
    "            # Find the closest row in flirtFeatures_30s_baselinecorrected by comparing timestamps\n",
    "            flirtFeatures_30s_baselinecorrected['time_diff'] = (flirtFeatures_30s_baselinecorrected['timestamp_flirt'] - timestamp).abs()\n",
    "\n",
    "            # Find the row with the smallest time difference\n",
    "            closest_row = flirtFeatures_30s_baselinecorrected.loc[flirtFeatures_30s_baselinecorrected['time_diff'].idxmin()]\n",
    "\n",
    "            # Reset index to avoid merging conflicts\n",
    "            button_row = button_row.to_frame().T.reset_index(drop=True)\n",
    "            closest_row = closest_row.to_frame().T.reset_index(drop=True)\n",
    "\n",
    "            # Merge rows properly (ensure both rows are 2D DataFrames)\n",
    "            merged_row = pd.concat([button_row, closest_row], axis=1)\n",
    "\n",
    "            # Calculate the time difference (timediff_flirtnk) in a new column\n",
    "            merged_row['timediff_flirtnk'] = (merged_row['window_start_time'] - merged_row['timestamp_flirt']).abs()\n",
    "\n",
    "            # Append the merged row to the list\n",
    "            merged_rows.append(merged_row)\n",
    "\n",
    "        # Convert the list of merged rows into a DataFrame (ensure it's 2D)\n",
    "        merged_features = pd.concat(merged_rows, ignore_index=True)\n",
    "\n",
    "        ''' old cleaning code, now moved to extendDataset\n",
    "        #### clean columns\n",
    "\n",
    "\n",
    "        # Optional: Drop 'time_diff' column if you don't need it\n",
    "        merged_features.drop(columns=['time_diff'], errors='ignore', inplace=True)\n",
    "\n",
    "        # Get the list of columns\n",
    "        cols = list(merged_features.columns)\n",
    "\n",
    "        # Find the index positions\n",
    "        timediff_idx = cols.index('timediff_flirtnk')\n",
    "        hr_mean_idx = cols.index('hr_mean')\n",
    "\n",
    "        # Remove 'timestamp_flirt' from its current position\n",
    "        cols.remove('timestamp_flirt')\n",
    "\n",
    "        # Insert 'timestamp_flirt' in the correct position (right after 'timediff_flirtnk')\n",
    "        cols.insert(timediff_idx + 1, 'timestamp_flirt')\n",
    "\n",
    "        # Reorder the DataFrame\n",
    "        merged_features = merged_features[cols]\n",
    "\n",
    "        # Rename the column\n",
    "        merged_features.rename(columns={'time_difference': 'timediff_buttonloc'}, inplace=True)\n",
    "\n",
    "        # Reorder columns to place 'timediff_buttonloc' between 'window_end_time' and 'timediff_flirtnk'\n",
    "        columns = list(merged_features.columns)\n",
    "        col_idx_window_end_time = columns.index('window_end_time')\n",
    "        col_idx_timediff_flirtnk = columns.index('timediff_flirtnk')\n",
    "\n",
    "        # Insert 'timediff_buttonloc' in the correct position\n",
    "        columns.insert(col_idx_timediff_flirtnk, columns.pop(columns.index('timediff_buttonloc')))\n",
    "        merged_features = merged_features[columns]\n",
    "\n",
    "        # Drop the unwanted columns\n",
    "        merged_features.drop(columns=['hr_mean.1', 'hr_stdev.1', 'hrv_mean.1', 'hrv_stdev.1'], errors='ignore', inplace=True)\n",
    "\n",
    "        # List of columns to rename\n",
    "        columns_to_rename = [\n",
    "            \"hr_mean\", \"hr_stdev\", \"hrv_mean\", \"hrv_mean\", \"hrv_stdev\", \"HRV_MeanNN\", \"HRV_SDNN\", \"HRV_SDANN1\", \n",
    "            \"HRV_SDNNI1\", \"HRV_SDANN2\", \"HRV_SDNNI2\", \"HRV_SDANN5\", \"HRV_SDNNI5\", \"HRV_RMSSD\", \"HRV_SDSD\", \n",
    "            \"HRV_CVNN\", \"HRV_CVSD\", \"HRV_MedianNN\", \"HRV_MadNN\", \"HRV_MCVNN\", \"HRV_IQRNN\", \"HRV_SDRMSSD\", \n",
    "            \"HRV_Prc20NN\", \"HRV_Prc80NN\", \"HRV_pNN50\", \"HRV_pNN20\", \"HRV_MinNN\", \"HRV_MaxNN\", \"HRV_HTI\", \n",
    "            \"HRV_TINN\", \"HRV_ULF\", \"HRV_VLF\", \"HRV_LF\", \"HRV_HF\", \"HRV_VHF\", \"HRV_TP\", \"HRV_LFHF\", \"HRV_LFn\", \n",
    "            \"HRV_HFn\", \"HRV_LnHF\", \"HRV_SD1\", \"HRV_SD2\", \"HRV_SD1SD2\", \"HRV_S\", \"HRV_CSI\", \"HRV_CVI\", \n",
    "            \"HRV_CSI_Modified\", \"HRV_PIP\", \"HRV_IALS\", \"HRV_PSS\", \"HRV_PAS\", \"HRV_GI\", \"HRV_SI\", \"HRV_AI\", \n",
    "            \"HRV_PI\", \"HRV_C1d\", \"HRV_C1a\", \"HRV_SD1d\", \"HRV_SD1a\", \"HRV_C2d\", \"HRV_C2a\", \"HRV_SD2d\", \n",
    "            \"HRV_SD2a\", \"HRV_Cd\", \"HRV_Ca\", \"HRV_SDNNd\", \"HRV_SDNNa\", \"HRV_DFA_alpha1\", \"HRV_MFDFA_alpha1_Width\", \n",
    "            \"HRV_MFDFA_alpha1_Peak\", \"HRV_MFDFA_alpha1_Mean\", \"HRV_MFDFA_alpha1_Max\", \"HRV_MFDFA_alpha1_Delta\", \n",
    "            \"HRV_MFDFA_alpha1_Asymmetry\", \"HRV_MFDFA_alpha1_Fluctuation\", \"HRV_MFDFA_alpha1_Increment\", \n",
    "            \"HRV_ApEn\", \"HRV_SampEn\", \"HRV_ShanEn\", \"HRV_FuzzyEn\", \"HRV_MSEn\", \"HRV_CMSEn\", \"HRV_RCMSEn\", \n",
    "            \"HRV_CD\", \"HRV_HFD\", \"HRV_KFD\", \"HRV_LZC\"\n",
    "        ]\n",
    "\n",
    "        # Create a dictionary mapping old names to new names with \"H10_\" prefix\n",
    "        rename_dict = {col: f\"H10_{col}\" for col in columns_to_rename}\n",
    "\n",
    "        # Rename columns\n",
    "        merged_features.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "        # List of columns to rename with \"E4_\" prefix\n",
    "        columns_to_rename_e4 = [\n",
    "            \"num_ibis\", \"hrv_mean_nni\", \"hrv_median_nni\", \"hrv_range_nni\", \"hrv_sdsd\", \"hrv_rmssd\", \"hrv_nni_50\", \n",
    "            \"hrv_pnni_50\", \"hrv_nni_20\", \"hrv_pnni_20\", \"hrv_cvsd\", \"hrv_sdnn\", \"hrv_cvnni\", \"hrv_mean_hr\", \n",
    "            \"hrv_min_hr\", \"hrv_max_hr\", \"hrv_std_hr\", \"hrv_total_power\", \"hrv_vlf\", \"hrv_lf\", \"hrv_hf\", \n",
    "            \"hrv_lf_hf_ratio\", \"hrv_lfnu\", \"hrv_hfnu\", \"hrv_mean\", \"hrv_mean\", \"hrv_std\", \"hrv_min\", \"hrv_max\", \n",
    "            \"hrv_ptp\", \"hrv_sum\", \"hrv_energy\", \"hrv_skewness\", \"hrv_kurtosis\", \"hrv_peaks\", \"hrv_rms\", \n",
    "            \"hrv_lineintegral\", \"hrv_n_above_mean\", \"hrv_n_below_mean\", \"hrv_n_sign_changes\", \"hrv_iqr\", \n",
    "            \"hrv_iqr_5_95\", \"hrv_pct_5\", \"hrv_pct_95\", \"hrv_entropy\", \"hrv_perm_entropy\", \"hrv_svd_entropy\", \n",
    "            \"eda_tonic_mean\", \"eda_tonic_std\", \"eda_tonic_min\", \"eda_tonic_max\", \"eda_tonic_ptp\", \"eda_tonic_sum\", \n",
    "            \"eda_tonic_energy\", \"eda_tonic_skewness\", \"eda_tonic_kurtosis\", \"eda_tonic_peaks\", \"eda_tonic_rms\", \n",
    "            \"eda_tonic_lineintegral\", \"eda_tonic_n_above_mean\", \"eda_tonic_n_below_mean\", \"eda_tonic_n_sign_changes\", \n",
    "            \"eda_tonic_iqr\", \"eda_tonic_iqr_5_95\", \"eda_tonic_pct_5\", \"eda_tonic_pct_95\", \"eda_tonic_entropy\", \n",
    "            \"eda_tonic_perm_entropy\", \"eda_tonic_svd_entropy\", \"eda_phasic_mean\", \"eda_phasic_std\", \"eda_phasic_min\", \n",
    "            \"eda_phasic_max\", \"eda_phasic_ptp\", \"eda_phasic_sum\", \"eda_phasic_energy\", \"eda_phasic_skewness\", \n",
    "            \"eda_phasic_kurtosis\", \"eda_phasic_peaks\", \"eda_phasic_rms\", \"eda_phasic_lineintegral\", \n",
    "            \"eda_phasic_n_above_mean\", \"eda_phasic_n_below_mean\", \"eda_phasic_n_sign_changes\", \"eda_phasic_iqr\", \n",
    "            \"eda_phasic_iqr_5_95\", \"eda_phasic_pct_5\", \"eda_phasic_pct_95\", \"eda_phasic_entropy\", \n",
    "            \"eda_phasic_perm_entropy\", \"eda_phasic_svd_entropy\", \"acc_acc_x_mean\", \"acc_acc_x_std\", \"acc_acc_x_min\", \n",
    "            \"acc_acc_x_max\", \"acc_acc_x_ptp\", \"acc_acc_x_sum\", \"acc_acc_x_energy\", \"acc_acc_x_skewness\", \n",
    "            \"acc_acc_x_kurtosis\", \"acc_acc_x_peaks\", \"acc_acc_x_rms\", \"acc_acc_x_lineintegral\", \n",
    "            \"acc_acc_x_n_above_mean\", \"acc_acc_x_n_below_mean\", \"acc_acc_x_n_sign_changes\", \"acc_acc_x_iqr\", \n",
    "            \"acc_acc_x_iqr_5_95\", \"acc_acc_x_pct_5\", \"acc_acc_x_pct_95\", \"acc_acc_x_entropy\", \"acc_acc_x_perm_entropy\", \n",
    "            \"acc_acc_x_svd_entropy\", \"acc_acc_y_mean\", \"acc_acc_y_std\", \"acc_acc_y_min\", \"acc_acc_y_max\", \n",
    "            \"acc_acc_y_ptp\", \"acc_acc_y_sum\", \"acc_acc_y_energy\", \"acc_acc_y_skewness\", \"acc_acc_y_kurtosis\", \n",
    "            \"acc_acc_y_peaks\", \"acc_acc_y_rms\", \"acc_acc_y_lineintegral\", \"acc_acc_y_n_above_mean\", \n",
    "            \"acc_acc_y_n_below_mean\", \"acc_acc_y_n_sign_changes\", \"acc_acc_y_iqr\", \"acc_acc_y_iqr_5_95\", \n",
    "            \"acc_acc_y_pct_5\", \"acc_acc_y_pct_95\", \"acc_acc_y_entropy\", \"acc_acc_y_perm_entropy\", \n",
    "            \"acc_acc_y_svd_entropy\", \"acc_acc_z_mean\", \"acc_acc_z_std\", \"acc_acc_z_min\", \"acc_acc_z_max\", \n",
    "            \"acc_acc_z_ptp\", \"acc_acc_z_sum\", \"acc_acc_z_energy\", \"acc_acc_z_skewness\", \"acc_acc_z_kurtosis\", \n",
    "            \"acc_acc_z_peaks\", \"acc_acc_z_rms\", \"acc_acc_z_lineintegral\", \"acc_acc_z_n_above_mean\", \n",
    "            \"acc_acc_z_n_below_mean\", \"acc_acc_z_n_sign_changes\", \"acc_acc_z_iqr\", \"acc_acc_z_iqr_5_95\", \n",
    "            \"acc_acc_z_pct_5\", \"acc_acc_z_pct_95\", \"acc_acc_z_entropy\", \"acc_acc_z_perm_entropy\", \n",
    "            \"acc_acc_z_svd_entropy\", \"acc_l2_mean\", \"acc_l2_std\", \"acc_l2_min\", \"acc_l2_max\", \"acc_l2_ptp\", \n",
    "            \"acc_l2_sum\", \"acc_l2_energy\", \"acc_l2_skewness\", \"acc_l2_kurtosis\", \"acc_l2_peaks\", \"acc_l2_rms\", \n",
    "            \"acc_l2_lineintegral\", \"acc_l2_n_above_mean\", \"acc_l2_n_below_mean\", \"acc_l2_n_sign_changes\", \n",
    "            \"acc_l2_iqr\", \"acc_l2_iqr_5_95\", \"acc_l2_pct_5\", \"acc_l2_pct_95\", \"acc_l2_entropy\", \"acc_l2_perm_entropy\", \n",
    "            \"acc_l2_svd_entropy\"\n",
    "            \n",
    "        ]\n",
    "\n",
    "        # Create a dictionary mapping old names to new names with \"E4_\" prefix\n",
    "        rename_dict_e4 = {col: f\"E4_{col}\" for col in columns_to_rename_e4}\n",
    "\n",
    "        # Rename columns\n",
    "        merged_features.rename(columns=rename_dict_e4, inplace=True)\n",
    "\n",
    "        # Reorder the DataFrame according to the new column order\n",
    "        merged_row = merged_row[cols]\n",
    "\n",
    "        # Define the order of movement\n",
    "        columns_to_move = ['timediff_flirtnk', 'timediff_buttonloc', 'timestamp_flirt']\n",
    "\n",
    "        # Get column index positions\n",
    "        col_idx_window_end_time = merged_features.columns.get_loc('window_end_time') + 1  # Position right after 'window_end_time'\n",
    "\n",
    "        # Remove columns from their original positions\n",
    "        remaining_columns = [col for col in merged_features.columns if col not in columns_to_move]\n",
    "\n",
    "        # Insert the columns in the correct place\n",
    "        new_columns = remaining_columns[:col_idx_window_end_time] + columns_to_move + remaining_columns[col_idx_window_end_time:]   \n",
    "\n",
    "        # Reorder dataframe\n",
    "        merged_features = merged_features[new_columns]\n",
    "\n",
    "        '''\n",
    "\n",
    "        # Save the corrected DataFrame to a new CSV file in the same folder\n",
    "        output_file_path = os.path.join(subdir, f\"{p_id}_ratingsFeatures_baselcorr_17-3.csv\")\n",
    "        merged_features.to_csv(output_file_path, index=None)\n",
    "        \n",
    "        print(f\"Saved merged features to: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded file: C:\\Users\\BootMR\\Documents\\data_export\\03\\03_ratingsFeatures_baselcorr_17-3.csv, Rows: 8\n",
      "Saved merged DataFrame to: C:\\Users\\BootMR\\Documents\\data_export\\all_ratingsflirtneurokit.csv, Total Rows: 8\n"
     ]
    }
   ],
   "source": [
    "## merge all individual files into 1 large\n",
    "\n",
    "# Define the root directory and the output file path\n",
    "\n",
    "output_file_path = os.path.join(parent_dir, \"all_ratingsflirtneurokit.csv\")\n",
    "\n",
    "# Initialize a list to store DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Iterate over all subfolders\n",
    "for subdir, _, files in os.walk(parent_dir):\n",
    "    # Skip the root directory itself\n",
    "    if subdir == parent_dir:\n",
    "        continue\n",
    "    \n",
    "    # Extract the subfolder name\n",
    "    subfolder_name = os.path.basename(subdir)\n",
    "    \n",
    "    # Check for files containing \"FlirtNkFeatures\" in the current subfolder\n",
    "    for file in files:\n",
    "        if \"ratingsFeatures_baselcorr\" in file:\n",
    "            file_path = os.path.join(subdir, file)\n",
    "            try:\n",
    "                # Read the file into a DataFrame\n",
    "                df = pd.read_csv(file_path)\n",
    "                \n",
    "                # Add the subfolder name as a new column\n",
    "                df['p_id'] = subfolder_name\n",
    "                \n",
    "                # Reorder the columns\n",
    "                cols = df.columns.tolist()\n",
    "                reordered_cols = ['p_id', 'rating', 'timestamp_button'] + [col for col in cols if col not in ['p_id', 'rating', 'timestamp_button']]\n",
    "                df = df[reordered_cols]\n",
    "                \n",
    "                # Append the DataFrame to the list\n",
    "                dataframes.append(df)\n",
    "                print(f\"Loaded file: {file_path}, Rows: {len(df)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file {file_path}: {e}\")\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "if dataframes:\n",
    "    merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "    \n",
    "    # Save the merged DataFrame to a CSV file\n",
    "    merged_df.to_csv(output_file_path, index=False)\n",
    "    print(f\"Saved merged DataFrame to: {output_file_path}, Total Rows: {len(merged_df)}\")\n",
    "else:\n",
    "    print(\"No files with 'FlirtNkFeatures' found in any subfolder.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
